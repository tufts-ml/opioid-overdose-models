{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/jyontika/Desktop/opioid-overdose-models/cook-county-data/cook-county-svi-data/'\n",
    "svi_2016_dir = os.path.join(data_dir,'cook-county-svi-2016.csv')\n",
    "svi_2018_dir = os.path.join(data_dir,'cook-county-svi-2018.csv')\n",
    "svi_2020_dir = os.path.join(data_dir,'cook-county-svi-2020.csv')\n",
    "\n",
    "svi_2016 = pd.read_csv(svi_2016_dir)\n",
    "svi_2018 = pd.read_csv(svi_2018_dir)\n",
    "svi_2020 = pd.read_csv(svi_2020_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "#retrieve cleaned data frames \n",
    "data_dir = '/Users/jyontika/Desktop/opioid-overdose-models/cook-county/cleaning-cook-county/'\n",
    "gdf_annual = pd.read_csv(f'{data_dir}/cook_county_gdf_year.csv')\n",
    "gdf_quarter = pd.read_csv(f'{data_dir}/cook_county_gdf_quarterly.csv')\n",
    "gdf_semi = pd.read_csv(f'{data_dir}/cook_county_gdf_semiannual.csv')\n",
    "\n",
    "#convert to gpd (was having trouble importing csv as gdf)\n",
    "gdf_annual['geometry'] = gdf_annual['geometry'].apply(wkt.loads)\n",
    "gdf_annual = gpd.GeoDataFrame(gdf_annual, geometry='geometry')\n",
    "gdf_annual.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_annual)\n",
    "\n",
    "gdf_quarter['geometry'] = gdf_quarter['geometry'].apply(wkt.loads)\n",
    "gdf_quarter = gpd.GeoDataFrame(gdf_quarter, geometry='geometry')\n",
    "gdf_quarter.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_quarter)\n",
    "\n",
    "gdf_semi['geometry'] = gdf_semi['geometry'].apply(wkt.loads)\n",
    "gdf_semi = gpd.GeoDataFrame(gdf_semi, geometry='geometry')\n",
    "gdf_semi.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_semi)\n",
    "\n",
    "gdf_annual['geoid'] = gdf_annual['geoid'].astype(str)\n",
    "gdf_semi['geoid'] = gdf_semi['geoid'].astype(str)\n",
    "gdf_quarter['geoid'] = gdf_quarter['geoid'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in SVI to gdf_annual data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_annual \n",
    "#initalize new columns with NAs\n",
    "\n",
    "gdf_annual = gdf_annual.assign(\n",
    "    RPL_THEME1 = float('nan'),\n",
    "    RPL_THEME2 = float('nan'),\n",
    "    RPL_THEME3 = float('nan'),\n",
    "    RPL_THEME4 = float('nan'),\n",
    "    RPL_THEMES = float('nan'))\n",
    "\n",
    "##should i match svi years with kyle, or change because i have different years?\n",
    "##2015, 2016 = svi_2016\n",
    "##2017, 2018 = svi_2018\n",
    "##2019, 2020, 2021, 2022 = svi_2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2016 to gdf_annual\n",
    "svi_2016['FIPS'] = svi_2016['FIPS'].astype(str)\n",
    "gdf_annual['geoid'] = gdf_annual['geoid'].str.strip()\n",
    "years_to_update = list(range(2015, 2017))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2015 <= year <= 2016\n",
    "for index, row in gdf_annual[gdf_annual['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2016[svi_2016['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_annual.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2018 to gdf_annual\n",
    "svi_2018['FIPS'] = svi_2018['FIPS'].astype(str)\n",
    "gdf_annual['geoid'] = gdf_annual['geoid'].str.strip()\n",
    "years_to_update = list(range(2017, 2019))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2017 <= year <= 2018\n",
    "for index, row in gdf_annual[gdf_annual['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2018[svi_2018['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_annual.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2020 to gdf_annual\n",
    "svi_2020['FIPS'] = svi_2020['FIPS'].astype(str)\n",
    "gdf_annual['geoid'] = gdf_annual['geoid'].str.strip()\n",
    "years_to_update = list(range(2019, 2023))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2019 <= year <= 2022\n",
    "for index, row in gdf_annual[gdf_annual['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2020[svi_2020['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_annual.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_annual.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping census tracts with zero population (lake)\n",
    "geoid_to_drop = ['17031990000', '17031381700', '17031980000', '17031980100']\n",
    "gdf_annual = gdf_annual[~gdf_annual['geoid'].isin(geoid_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique geoids with NA values: 44\n",
      "Unique geoids with NA values:\n",
      "['17031320101' '17031320102' '17031330101' '17031330102' '17031330103'\n",
      " '17031380600' '17031460800' '17031490200' '17031612200' '17031630600'\n",
      " '17031803613' '17031803614' '17031803615' '17031803616' '17031804203'\n",
      " '17031804204' '17031804312' '17031804313' '17031804314' '17031804315'\n",
      " '17031804316' '17031804512' '17031804513' '17031804514' '17031806005'\n",
      " '17031806006' '17031809401' '17031809402' '17031820203' '17031820204'\n",
      " '17031824124' '17031824125' '17031824126' '17031824127' '17031824128'\n",
      " '17031824129' '17031824508' '17031824509' '17031828507' '17031828508'\n",
      " '17031829903' '17031829904' '17031844600' '17031844700']\n"
     ]
    }
   ],
   "source": [
    "# check for NA vals in each row and sum the number of NA values\n",
    "na_counts = gdf_annual.isna().sum(axis=1)\n",
    "\n",
    "# filter rows with NA values\n",
    "rows_with_na = gdf_annual[na_counts > 0]\n",
    "\n",
    "geoid_values_with_na = rows_with_na['geoid'].unique()\n",
    "\n",
    "# Print the count and the unique list of geoids with NA values\n",
    "print(f\"Number of unique geoids with NA values: {len(geoid_values_with_na)}\")\n",
    "print(\"Unique geoids with NA values:\")\n",
    "print(geoid_values_with_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate NAs for geoids that are only missing *some* rows\n",
    "\n",
    "columns_to_fill = ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES']\n",
    "\n",
    "for geoid_value in geoid_values_with_na:\n",
    "    rows_for_geoid = gdf_annual[gdf_annual['geoid'] == geoid_value]\n",
    "    rows_with_values = rows_for_geoid.dropna(subset=columns_to_fill, how='any')\n",
    "    \n",
    "    if not rows_with_values.empty:\n",
    "        values_to_fill = rows_with_values.iloc[0][columns_to_fill].to_dict()\n",
    "    \n",
    "        gdf_annual.loc[gdf_annual['geoid'] == geoid_value, columns_to_fill] = gdf_annual.loc[\n",
    "            gdf_annual['geoid'] == geoid_value, columns_to_fill\n",
    "        ].fillna(values_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/w0g9cprs0498yrq5bc98933m0000gn/T/ipykernel_25722/3343451090.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gdf_annual.rename(columns={\n"
     ]
    }
   ],
   "source": [
    "#rename columns to match MA\n",
    "\n",
    "gdf_annual.rename(columns={\n",
    "    'RPL_THEME1': 'theme_1_pc',\n",
    "    'RPL_THEME2': 'theme_2_pc',\n",
    "    'RPL_THEME3': 'theme_3_pc',\n",
    "    'RPL_THEME4': 'theme_4_pc',\n",
    "    'RPL_THEMES': 'svi_pctile'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOIDs with NAs:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#three geoids have NAs \n",
    "# ['17031381700' '17031980000' '17031980100']\n",
    "\n",
    "gdf_annual = gdf_annual.replace(-999.0000, np.nan)\n",
    "\n",
    "geoids_with_nas = gdf_annual[gdf_annual.isna().any(axis=1)]['geoid'].unique()\n",
    "print(\"GEOIDs with NAs:\")\n",
    "print(geoids_with_nas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in SVI to gdf_quarter data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_quarter\n",
    "#initalize new columns with NAs\n",
    "\n",
    "gdf_quarter = gdf_quarter.assign(\n",
    "    RPL_THEME1 = float('nan'),\n",
    "    RPL_THEME2 = float('nan'),\n",
    "    RPL_THEME3 = float('nan'),\n",
    "    RPL_THEME4 = float('nan'),\n",
    "    RPL_THEMES = float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2016 to gdf_quarter\n",
    "svi_2016['FIPS'] = svi_2016['FIPS'].astype(str)\n",
    "gdf_quarter['geoid'] = gdf_quarter['geoid'].str.strip()\n",
    "years_to_update = list(range(2015, 2017))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2015 <= year <= 2016\n",
    "for index, row in gdf_quarter[gdf_quarter['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2016[svi_2016['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_quarter.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2018 to gdf_quarter\n",
    "svi_2018['FIPS'] = svi_2018['FIPS'].astype(str)\n",
    "gdf_quarter['geoid'] = gdf_quarter['geoid'].str.strip()\n",
    "years_to_update = list(range(2017, 2019))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2017 <= year <= 2018\n",
    "for index, row in gdf_quarter[gdf_quarter['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2018[svi_2018['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_quarter.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2020 to gdf_quarter\n",
    "svi_2020['FIPS'] = svi_2020['FIPS'].astype(str)\n",
    "gdf_quarter['geoid'] = gdf_quarter['geoid'].str.strip()\n",
    "years_to_update = list(range(2019, 2023))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2019 <= year <= 2022\n",
    "for index, row in gdf_quarter[gdf_quarter['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2020[svi_2020['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_quarter.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_quarter.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping census tracts with zero population (lake)\n",
    "gdf_quarter = gdf_quarter[~gdf_quarter['geoid'].isin(geoid_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate NAs for GEOIDs that are only missing *some* rows\n",
    "na_counts = gdf_quarter.isna().sum(axis=1)\n",
    "rows_with_na = gdf_quarter[na_counts > 0]\n",
    "geoid_values_with_na = rows_with_na['geoid'].unique()\n",
    "\n",
    "columns_to_fill = ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES']\n",
    "\n",
    "for geoid_value in geoid_values_with_na:\n",
    "    rows_for_geoid = gdf_quarter[gdf_quarter['geoid'] == geoid_value]\n",
    "    rows_with_values = rows_for_geoid.dropna(subset=columns_to_fill, how='any')\n",
    "    \n",
    "    if not rows_with_values.empty:\n",
    "        values_to_fill = rows_with_values.iloc[0][columns_to_fill].to_dict()\n",
    "    \n",
    "        gdf_quarter.loc[gdf_quarter['geoid'] == geoid_value, columns_to_fill] = gdf_quarter.loc[\n",
    "            gdf_quarter['geoid'] == geoid_value, columns_to_fill\n",
    "        ].fillna(values_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to match kyle\n",
    "\n",
    "gdf_quarter.rename(columns={\n",
    "    'RPL_THEME1': 'theme_1_pc',\n",
    "    'RPL_THEME2': 'theme_2_pc',\n",
    "    'RPL_THEME3': 'theme_3_pc',\n",
    "    'RPL_THEME4': 'theme_4_pc',\n",
    "    'RPL_THEMES': 'svi_pctile'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geoid with NAs:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#three GEOIDs have NAs \n",
    "# ['17031381700' '17031980000' '17031980100']\n",
    "\n",
    "gdf_quarter = gdf_quarter.replace(-999.0000, np.nan)\n",
    "geoids_with_nas = gdf_quarter[gdf_quarter.isna().any(axis=1)]['geoid'].unique()\n",
    "print(\"geoid with NAs:\")\n",
    "print(geoids_with_nas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in SVI to gdf_semi data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_semi\n",
    "#initalize new columns with NAs\n",
    "\n",
    "gdf_semi = gdf_semi.assign(\n",
    "    RPL_THEME1 = float('nan'),\n",
    "    RPL_THEME2 = float('nan'),\n",
    "    RPL_THEME3 = float('nan'),\n",
    "    RPL_THEME4 = float('nan'),\n",
    "    RPL_THEMES = float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2016 to gdf_semi\n",
    "svi_2016['FIPS'] = svi_2016['FIPS'].astype(str)\n",
    "gdf_semi['geoid'] = gdf_semi['geoid'].str.strip()\n",
    "years_to_update = list(range(2015, 2017))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2015 <= year <= 2016\n",
    "for index, row in gdf_semi[gdf_semi['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2016[svi_2016['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_semi.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2018 to gdf_semi\n",
    "svi_2018['FIPS'] = svi_2018['FIPS'].astype(str)\n",
    "gdf_semi['geoid'] = gdf_semi['geoid'].str.strip()\n",
    "years_to_update = list(range(2017, 2019))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2017 <= year <= 2018\n",
    "for index, row in gdf_semi[gdf_semi['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2018[svi_2018['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_semi.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in svi_2020 to gdf_semi\n",
    "svi_2020['FIPS'] = svi_2020['FIPS'].astype(str)\n",
    "gdf_semi['geoid'] = gdf_semi['geoid'].str.strip()\n",
    "years_to_update = list(range(2019, 2023))\n",
    "\n",
    "# Iterate over each row in gdf_annual where 2019 <= year <= 2022\n",
    "for index, row in gdf_semi[gdf_semi['year'].isin(years_to_update)].iterrows():\n",
    "    geoid_value = row['geoid']\n",
    "    matching_row = svi_2020[svi_2020['FIPS'] == geoid_value]\n",
    "    \n",
    "    # If a matching row is found, update the corresponding columns in gdf_annual\n",
    "    if not matching_row.empty:\n",
    "        gdf_semi.loc[index, 'RPL_THEME1'] = matching_row['RPL_THEME1'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME2'] = matching_row['RPL_THEME2'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME3'] = matching_row['RPL_THEME3'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEME4'] = matching_row['RPL_THEME4'].values[0]\n",
    "        gdf_semi.loc[index, 'RPL_THEMES'] = matching_row['RPL_THEMES'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping census tracts with zero population (lake)\n",
    "gdf_semi = gdf_semi[~gdf_semi['geoid'].isin(geoid_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate NAs for GEOIDs that are only missing *some* rows\n",
    "na_counts = gdf_semi.isna().sum(axis=1)\n",
    "rows_with_na = gdf_semi[na_counts > 0]\n",
    "geoid_values_with_na = rows_with_na['geoid'].unique()\n",
    "\n",
    "columns_to_fill = ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES']\n",
    "\n",
    "for geoid_value in geoid_values_with_na:\n",
    "    rows_for_geoid = gdf_semi[gdf_semi['geoid'] == geoid_value]\n",
    "    rows_with_values = rows_for_geoid.dropna(subset=columns_to_fill, how='any')\n",
    "    \n",
    "    if not rows_with_values.empty:\n",
    "        values_to_fill = rows_with_values.iloc[0][columns_to_fill].to_dict()\n",
    "    \n",
    "        gdf_semi.loc[gdf_semi['geoid'] == geoid_value, columns_to_fill] = gdf_semi.loc[\n",
    "            gdf_semi['geoid'] == geoid_value, columns_to_fill\n",
    "        ].fillna(values_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/w0g9cprs0498yrq5bc98933m0000gn/T/ipykernel_25722/2328704100.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gdf_semi.rename(columns={\n"
     ]
    }
   ],
   "source": [
    "#rename columns to match kyle\n",
    "\n",
    "gdf_semi.rename(columns={\n",
    "    'RPL_THEME1': 'theme_1_pc',\n",
    "    'RPL_THEME2': 'theme_2_pc',\n",
    "    'RPL_THEME3': 'theme_3_pc',\n",
    "    'RPL_THEME4': 'theme_4_pc',\n",
    "    'RPL_THEMES': 'svi_pctile'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOIDs with NAs:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#three GEOIDs have NAs \n",
    "# ['17031381700' '17031980000' '17031980100']\n",
    "\n",
    "gdf_semi = gdf_semi.replace(-999.0000, np.nan)\n",
    "geoids_with_nas = gdf_semi[gdf_semi.isna().any(axis=1)]['geoid'].unique()\n",
    "print(\"GEOIDs with NAs:\")\n",
    "print(geoids_with_nas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking NAs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs in each column:\n",
      "            gdf_annual  gdf_semi  gdf_quarter\n",
      "geoid              0.0       0.0          0.0\n",
      "year               0.0       0.0          0.0\n",
      "deaths             0.0       0.0          0.0\n",
      "STATEFP            0.0       0.0          0.0\n",
      "COUNTYFP           0.0       0.0          0.0\n",
      "TRACTCE            0.0       0.0          0.0\n",
      "NAME               0.0       0.0          0.0\n",
      "NAMELSAD           0.0       0.0          0.0\n",
      "MTFCC              0.0       0.0          0.0\n",
      "FUNCSTAT           0.0       0.0          0.0\n",
      "ALAND              0.0       0.0          0.0\n",
      "AWATER             0.0       0.0          0.0\n",
      "lat                0.0       0.0          0.0\n",
      "lon                0.0       0.0          0.0\n",
      "geometry           0.0       0.0          0.0\n",
      "timestep           0.0       0.0          0.0\n",
      "theme_1_pc         0.0       0.0          0.0\n",
      "theme_2_pc         0.0       0.0          0.0\n",
      "theme_3_pc         0.0       0.0          0.0\n",
      "theme_4_pc         0.0       0.0          0.0\n",
      "svi_pctile         0.0       0.0          0.0\n",
      "semiannual         NaN       0.0          NaN\n",
      "season             NaN       0.0          NaN\n",
      "quarter            NaN       NaN          0.0\n",
      "\n",
      "Total number of NAs:\n",
      "gdf_annual: 0\n",
      "gdf_semi: 0\n",
      "gdf_quarter: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NAs in each DataFrame\n",
    "na_counts_annual = gdf_annual.isna().sum()\n",
    "na_counts_semi = gdf_semi.isna().sum()\n",
    "na_counts_quarter = gdf_quarter.isna().sum()\n",
    "\n",
    "# Concatenate the na_counts Series horizontally\n",
    "na_counts_combined = pd.concat([na_counts_annual, na_counts_semi, na_counts_quarter], axis=1)\n",
    "na_counts_combined.columns = ['gdf_annual', 'gdf_semi', 'gdf_quarter']\n",
    "\n",
    "# Total number of NAs in each DataFrame\n",
    "total_na_count_annual = gdf_annual.isna().sum().sum()\n",
    "total_na_count_semi = gdf_semi.isna().sum().sum()\n",
    "total_na_count_quarter = gdf_quarter.isna().sum().sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of NAs in each column:\")\n",
    "print(na_counts_combined)\n",
    "\n",
    "print(\"\\nTotal number of NAs:\")\n",
    "print(\"gdf_annual:\", total_na_count_annual)\n",
    "print(\"gdf_semi:\", total_na_count_semi)\n",
    "print(\"gdf_quarter:\", total_na_count_quarter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/jyontika/Desktop/opioid-overdose-models/cook-county/cleaning-cook-county/'\n",
    "gdf_annual.to_csv(f'{data_dir}/cook_county_gdf_year.csv', index=False)\n",
    "gdf_quarter.to_csv(f'{data_dir}/cook_county_gdf_quarterly.csv', index=False)\n",
    "gdf_semi.to_csv(f'{data_dir}/cook_county_gdf_semiannual.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
