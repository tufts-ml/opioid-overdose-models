{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r gdf_annual_with_svi\n",
    "%store -r gdf_quarter_with_svi\n",
    "%store -r gdf_semi_with_svi\n",
    "\n",
    "gdf_annual = gdf_annual_with_svi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geoid', 'year', 'deaths', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'NAME',\n",
       "       'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'lat', 'lon',\n",
       "       'geometry', 'timestep', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc',\n",
       "       'theme_4_pc', 'svi_pctile'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/CASTNet/hughes-CASTNet/Data/Chicago/'\n",
    "gdf_annual.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add static feature as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_data = []\n",
    "\n",
    "for geoid in gdf_annual['geoid'].unique():\n",
    "    geoid_df = gdf_annual[gdf_annual['geoid'] == geoid]\n",
    "    \n",
    "    # Extract the lat and lon values \n",
    "    lat = geoid_df['lat'].values[0]\n",
    "    lon = geoid_df['lon'].values[0]\n",
    "    \n",
    "    # append data\n",
    "    geoid_data.append([geoid, lat, lon])\n",
    "\n",
    "\n",
    "static_features = pd.DataFrame(geoid_data)\n",
    "static_features = static_features.transpose()\n",
    "csv_path = os.path.join(data_path, 'static_features.csv')\n",
    "static_features.to_csv(csv_path, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_series = gdf_annual['geoid'].unique()\n",
    "txt_path = os.path.join(data_path, 'locations.txt')\n",
    "with open(txt_path, 'w') as file:\n",
    " for index, geoid in enumerate(geoid_series, start=1):\n",
    "        file.write(f\"{index}\\t{geoid}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SVI as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVI  dynamic features organization\n",
    "svi_cols = ['geoid', 'year', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc', 'svi_pctile']\n",
    "gdf_subset = gdf_annual[svi_cols]\n",
    "gdf_pivoted = gdf_subset.pivot(index='geoid', columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_geoids = len(gdf_pivoted.index)\n",
    "num_years = len(gdf_pivoted.columns.levels[1])\n",
    "num_values = len(svi_cols[2:])\n",
    "numpy_3d_array = gdf_pivoted.values.reshape(num_unique_geoids, num_years, num_values)\n",
    "svi_path = os.path.join(data_path, 'svi.pkl')\n",
    "with open(svi_path, \"wb\") as file:\n",
    "    pickle.dump(numpy_3d_array, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "\n",
    "for loc in range(0, numpy_3d_array.shape[0]):\n",
    "        new_time_idx = 0\n",
    "        for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "            start_idx = i\n",
    "            end_idx = i + 1\n",
    "            if end_idx > numpy_3d_array.shape[1]:\n",
    "                end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "            svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "            new_time_idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "num_time_slots = svi_agg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 8, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_3d_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add overdose as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overdoses pickle file is an array (# days, # locations) shows # of deaths per DAY in every location??\n",
    "# (8, 1327)\n",
    "\n",
    "overdose_data = gdf_annual.groupby(['year', 'geoid'])['deaths'].sum().reset_index()\n",
    "overdoses_array = overdose_data.pivot_table(index='year', columns='geoid', values='deaths').to_numpy()\n",
    "overdose_path = os.path.join(data_path, 'overdose.pkl')\n",
    "with open(overdose_path, \"wb\") as file:\n",
    "    pickle.dump(overdoses_array, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(overdoses_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "overdoses_array = np.swapaxes(overdoses_array, 1, 0)\n",
    "\n",
    "overdose_agg = np.zeros(shape=(overdoses_array.shape[0], num_agg_slots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overdose_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     end_idx \u001b[39m=\u001b[39m numpy_3d_array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m svi_agg[loc, new_time_idx] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(numpy_3d_array[loc, start_idx:end_idx], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m overdose_agg[loc, new_time_idx] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(overdoses_array[loc, start_idx:end_idx])\n\u001b[1;32m     11\u001b[0m new_time_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 0 with size 8"
     ]
    }
   ],
   "source": [
    "# for loc in range(0, numpy_3d_array.shape[0]):\n",
    "#         new_time_idx = 0\n",
    "#         for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "#             start_idx = i\n",
    "#             end_idx = i + 1\n",
    "#             if end_idx > numpy_3d_array.shape[1]:\n",
    "#                 end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "#             svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "#             overdose_agg[loc, new_time_idx] = np.sum(overdoses_array[loc, start_idx:end_idx])\n",
    "#             new_time_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distances as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "data_dir = '/Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/cook-county/cleaning-cook-county/files/'\n",
    "tl_shape_path = os.path.join(data_dir, 'tl_2021_17_tract/tl_2021_17_tract.shp')\n",
    "tl_gdf = gpd.read_file(tl_shape_path)\n",
    "tl_gdf = tl_gdf[tl_gdf['COUNTYFP']=='031']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoid_to_drop = ['17031990000', '17031381700', '17031980000', '17031980100']\n",
    "tl_gdf = tl_gdf[~tl_gdf['GEOID'].isin(geoid_to_drop)]\n",
    "\n",
    "unique_locations_df = tl_gdf[['INTPTLAT', 'INTPTLON']].drop_duplicates()\n",
    "unique_locations_df['INTPTLAT'] = pd.to_numeric(unique_locations_df['INTPTLAT'])\n",
    "unique_locations_df['INTPTLON'] = pd.to_numeric(unique_locations_df['INTPTLON'])\n",
    "unique_locations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371.0  # Earth's radius in kilometers\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    dLat = lat2_rad - lat1_rad\n",
    "    dLon = lon2_rad - lon1_rad\n",
    "    a = math.sin(dLat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dLon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mention lat and lon points to kyle\n",
    "#find representative center point for each tract \n",
    "#calculates  distance matrix for a set of unique locations based on their latitude and longitude coordinates \n",
    "#average distance between deaths in tract A, deaths in tract B --> grab the pairs and compute haversine \n",
    "#distance between tract A and tractB depends on individual death locations, using average pairwise-distance between deaths in A and deaths in B\n",
    "distance_matrix = []\n",
    "\n",
    "#check documentation and magnitude on distances\n",
    "\n",
    "for i, row_i in unique_locations_df.iterrows():\n",
    "    #for each row, initialize empty list row_distances to store  distances between row_i and all other locations\n",
    "    row_distances = []\n",
    "    for j, row_j in unique_locations_df.iterrows():\n",
    "        if i != j:  # avoid calculating distance with itself\n",
    "            distance = haversine(row_i['INTPTLAT'], row_i['INTPTLON'], row_j['INTPTLAT'], row_j['INTPTLON'])\n",
    "            row_distances.append(distance)\n",
    "        else:\n",
    "            row_distances.append(0)  # distance with itself is 0\n",
    "    distance_matrix.append(row_distances)\n",
    "\n",
    "distances_csv_path = os.path.join(data_path, 'distances.csv')\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix)\n",
    "distance_matrix_df.to_csv(distances_csv_path, index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
