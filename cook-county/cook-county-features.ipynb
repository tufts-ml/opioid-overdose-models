{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r gdf_annual_with_svi\n",
    "%store -r gdf_quarter_with_svi\n",
    "%store -r gdf_semi_with_svi\n",
    "\n",
    "gdf_annual = gdf_annual_with_svi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geoid', 'year', 'deaths', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'NAME',\n",
       "       'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'lat', 'lon',\n",
       "       'geometry', 'timestep', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc',\n",
       "       'theme_4_pc', 'svi_pctile'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/CASTNet/hughes-CASTNet/Data/Chicago/'\n",
    "gdf_annual.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add static feature as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_data = []\n",
    "\n",
    "for geoid in gdf_annual['geoid'].unique():\n",
    "    geoid_df = gdf_annual[gdf_annual['geoid'] == geoid]\n",
    "    \n",
    "    # Extract the lat and lon values \n",
    "    lat = geoid_df['lat'].values[0]\n",
    "    lon = geoid_df['lon'].values[0]\n",
    "    \n",
    "    # append data\n",
    "    geoid_data.append([geoid, lat, lon])\n",
    "\n",
    "\n",
    "static_features = pd.DataFrame(geoid_data)\n",
    "static_features = static_features.transpose()\n",
    "csv_path = os.path.join(data_path, 'static_features.csv')\n",
    "static_features.to_csv(csv_path, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_series = gdf_annual['geoid'].unique()\n",
    "txt_path = os.path.join(data_path, 'locations.txt')\n",
    "with open(txt_path, 'w') as file:\n",
    " for index, geoid in enumerate(geoid_series, start=1):\n",
    "        file.write(f\"{index}\\t{geoid}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SVI as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVI  dynamic features organization\n",
    "svi_cols = ['geoid', 'year', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc', 'svi_pctile']\n",
    "gdf_subset = gdf_annual[svi_cols]\n",
    "gdf_pivoted = gdf_subset.pivot(index='geoid', columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_geoids = len(gdf_pivoted.index)\n",
    "num_years = len(gdf_pivoted.columns.levels[1])\n",
    "num_values = len(svi_cols[2:])\n",
    "numpy_3d_array = gdf_pivoted.values.reshape(num_unique_geoids, num_years, num_values)\n",
    "svi_path = os.path.join(data_path, 'svi.pkl')\n",
    "with open(svi_path, \"wb\") as file:\n",
    "    pickle.dump(numpy_3d_array, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "\n",
    "for loc in range(0, numpy_3d_array.shape[0]):\n",
    "        new_time_idx = 0\n",
    "        for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "            start_idx = i\n",
    "            end_idx = i + 1\n",
    "            if end_idx > numpy_3d_array.shape[1]:\n",
    "                end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "            svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "            new_time_idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "num_time_slots = svi_agg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1327, 8, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_3d_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add overdose as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overdoses pickle file is an array (# days, # locations) shows # of deaths per DAY in every location??\n",
    "# (8, 1327)\n",
    "\n",
    "overdose_data = gdf_annual.groupby(['year', 'geoid'])['deaths'].sum().reset_index()\n",
    "overdoses_array = overdose_data.pivot_table(index='year', columns='geoid', values='deaths').to_numpy()\n",
    "overdose_path = os.path.join(data_path, 'overdose.pkl')\n",
    "with open(overdose_path, \"wb\") as file:\n",
    "    pickle.dump(overdoses_array, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distances as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations_df = gdf_annual[['lat', 'lon']].drop_duplicates()\n",
    "unique_locations_df['lat'] = pd.to_numeric(unique_locations_df['lat'])\n",
    "unique_locations_df['lon'] = pd.to_numeric(unique_locations_df['lon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "  \"\"\"Calculates the distance between two points using the Haversine formula.\"\"\"\n",
    "  R = 6371.01 # Earth radius in kilometers\n",
    "  dLat = math.radians(lat2 - lat1)\n",
    "  dLon = math.radians(lon2 - lon1)\n",
    "  a = math.sin(dLat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dLon / 2)**2\n",
    "  c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "  return R * c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates  distance matrix for a set of unique locations based on their latitude and longitude coordinates \n",
    "\n",
    "distance_matrix = []\n",
    "\n",
    "for i, row_i in unique_locations_df.iterrows():\n",
    "    #for each row, initialize empty list row_distances to store  distances between row_i and all other locations\n",
    "    row_distances = []\n",
    "    for j, row_j in unique_locations_df.iterrows():\n",
    "        if i != j:  # avoid calculating distance with itself\n",
    "            distance = haversine(row_i['lat'], row_i['lon'], row_j['lat'], row_j['lon'])\n",
    "            row_distances.append(distance)\n",
    "        else:\n",
    "            row_distances.append(0)  # distance with itself is 0\n",
    "    distance_matrix.append(row_distances)\n",
    "\n",
    "distances_csv_path = os.path.join(data_path, 'distances.csv')\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix)\n",
    "distance_matrix_df.to_csv(distances_csv_path, index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
