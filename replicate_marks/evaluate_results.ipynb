{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import IndexSlice as idx\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "def fast_bpr(true_val, pred_val, K=100, bootstrap_samples=1000):\n",
    "    \"\"\"\n",
    "\n",
    "    :param true_val: Pandas dataframe indexed on location\n",
    "    :param pred_val: Pandas dataframe indexed on location\n",
    "    :param K: Number of locations to consider\n",
    "    :param bootstrap_samples: Number of samples to take when evaluating ties\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    top_K_predicted = pred_val.sort_values(ascending=False).iloc[:K]\n",
    "    top_K_true = true_val.sort_values(ascending=False).iloc[:K]\n",
    "\n",
    "    # Now we check for ties\n",
    "    undisputed_top_predicted = top_K_predicted[top_K_predicted > top_K_predicted.min()]\n",
    "    num_tied_spots = K - len(undisputed_top_predicted)\n",
    "    undisputed_top_true = top_K_true[top_K_true > top_K_true.min()]\n",
    "    num_true_ties = K - len(undisputed_top_true)\n",
    "\n",
    "    tied_top_predicted = pred_val[pred_val == top_K_predicted.min()]\n",
    "    tied_top_true = true_val[true_val == top_K_true.min()]\n",
    "\n",
    "    # now randomly choose locations from the tied spots\n",
    "    bootstrapped_tied_indices = np.random.choice(tied_top_predicted.index, (bootstrap_samples, num_tied_spots))\n",
    "    undisputed_pred_idx = undisputed_top_predicted.index.values\n",
    "    bootstrapped_all_indices = [np.concatenate((undisputed_pred_idx, bootstrap_index))\n",
    "                                for bootstrap_index in bootstrapped_tied_indices]\n",
    "\n",
    "\n",
    "    denominator =  top_K_true.sum()\n",
    "    numerators = [true_val[indicies].sum() for indicies in bootstrapped_all_indices]\n",
    "\n",
    "    bootstrapped_ratio = np.mean([numerator / denominator\n",
    "                                  for numerator in numerators])\n",
    "\n",
    "    return bootstrapped_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"/cluster/tufts/hugheslab/datasets/NSF_OD/results_202308_pipeline/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>geoid</th>\n",
       "      <th>year</th>\n",
       "      <th>observed</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25001010100</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.206030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25001010100</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.284624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25001010100</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>25001010100</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.345503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25001010100</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.345062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14575</th>\n",
       "      <td>14576</td>\n",
       "      <td>25027761402</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>14577</td>\n",
       "      <td>25027761402</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14577</th>\n",
       "      <td>14578</td>\n",
       "      <td>25027761402</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.348472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14578</th>\n",
       "      <td>14579</td>\n",
       "      <td>25027761402</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.437946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>14580</td>\n",
       "      <td>25027761402</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.281288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14580 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        geoid  year  observed  predicted\n",
       "0               1  25001010100  2013         0   0.206030\n",
       "1               2  25001010100  2014         1   0.284624\n",
       "2               3  25001010100  2015         0   0.369421\n",
       "3               4  25001010100  2016         0   0.345503\n",
       "4               5  25001010100  2017         0   0.345062\n",
       "...           ...          ...   ...       ...        ...\n",
       "14575       14576  25027761402  2017         0   0.307114\n",
       "14576       14577  25027761402  2018         1   0.296427\n",
       "14577       14578  25027761402  2019         0   0.348472\n",
       "14578       14579  25027761402  2020         0   0.437946\n",
       "14579       14580  25027761402  2021         1   0.281288\n",
       "\n",
       "[14580 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_locations=250\n",
    "bpr_uncertainty_samples=50\n",
    "seed=360\n",
    "num_locations = len(results['geoid'].unique())\n",
    "\n",
    "num_sampled = num_locations - removed_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "results_over_time = []\n",
    "true_deaths = []\n",
    "output_deaths=[]\n",
    "for year in range(2020, 2021+1):\n",
    "    evaluation_deaths = results[results['year'] == year]['observed']\n",
    "    predicted_deaths = results[results['year'] == year]['predicted']\n",
    "    true_deaths.append(evaluation_deaths)\n",
    "    output_deaths.append(predicted_deaths)\n",
    "\n",
    "    results_over_samples = []\n",
    "    for _ in range(bpr_uncertainty_samples):\n",
    "        sampled_indicies = rng.choice(range(num_locations), size=num_sampled, replace=False)\n",
    "        results_over_samples.append(fast_bpr(evaluation_deaths.iloc[sampled_indicies], predicted_deaths.iloc[sampled_indicies]))\n",
    "\n",
    "    results_over_time.append(results_over_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroes model (Mean, 95% CI): 62.0,\n",
      "      (60.3-\n",
      "       64.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bpr_samples_both_years = (np.array(results_over_time[0]) + \\\n",
    "                          np.array(results_over_time[1]))/2\n",
    "                        \n",
    "print(f\"\"\"Zeroes model (Mean, 95% CI): {np.mean(bpr_samples_both_years)*100:.1f},\n",
    "      ({np.percentile(bpr_samples_both_years,2.5)*100:.1f}-\n",
    "       {np.percentile(bpr_samples_both_years,97.5)*100:.1f})\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results, mae_results = calculate_metrics(true_deaths, output_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Zeroes Model (Mean, 95% CI): 1.31, (1.27-1.35)\n",
      "MAE for Zeroes Model (Mean, 95% CI): 0.92, (0.89-0.95)\n"
     ]
    }
   ],
   "source": [
    "rmse_mean, rmse_conf_interval = rmse_results\n",
    "mae_mean, mae_conf_interval = mae_results\n",
    "\n",
    "print_results(\"RMSE for Zeroes Model\", rmse_mean, rmse_conf_interval)\n",
    "print_results(\"MAE for Zeroes Model\", mae_mean, mae_conf_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(evaluation_deaths, predicted_deaths, \n",
    "                      num_locations_removed = 250, confidence_level=0.95, seed=360):\n",
    "        \"\"\"\n",
    "        @Return: joint RMSE and joint MAE alongside confidence interval\n",
    "        @param: evaluation_deaths pulled from multiindexed_gdf, not sampled yet\n",
    "        @param: predicted_deaths - corresponding model returns, already sampled\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed=seed) \n",
    "        num_years = len(evaluation_deaths)\n",
    "        num_uncertainty_samples = len(predicted_deaths) // num_years\n",
    "\n",
    "\n",
    "        #make sure each element in evaluation_deaths is of same len, set num_locations to that len\n",
    "        lengths = [len(sub_list) for sub_list in evaluation_deaths]\n",
    "        if all(length == lengths[0] for length in lengths):\n",
    "             num_locations = lengths[0] #1328 for cook county, 1620 for MA\n",
    "        \n",
    "        num_sampled = num_locations - num_locations_removed \n",
    "\n",
    "        #initialize lists to store values \n",
    "        mae_over_samples = [] \n",
    "        rmse_over_samples = []\n",
    "\n",
    "        #calculate metrics for each year across diff. samples of predicted values and actual values\n",
    "        for i in range(num_years): \n",
    "\n",
    "            sampled_indices = rng.choice(range(num_locations), size=num_sampled, replace=False)\n",
    "            current_eval_deaths = evaluation_deaths[i].iloc[sampled_indices]\n",
    "            current_predicted_deaths = predicted_deaths[i].iloc[sampled_indices]\n",
    "\n",
    "            #for test time 1, go through first half of predicted_deaths\n",
    "            if i == 0: \n",
    "                for _ in range(num_uncertainty_samples):\n",
    "                    mae_over_samples.append(mean_absolute_error(current_eval_deaths, current_predicted_deaths))\n",
    "                    rmse_over_samples.append(sqrt(mean_squared_error(current_eval_deaths, current_predicted_deaths)))\n",
    "\n",
    "            #for test time 2, go through second half of predicted_deaths\n",
    "            else:\n",
    "                upper_bound = len(evaluation_deaths)*num_uncertainty_samples \n",
    "                for _ in range(num_uncertainty_samples, upper_bound):\n",
    "                    mae_over_samples.append(mean_absolute_error(current_eval_deaths, current_predicted_deaths))\n",
    "                    rmse_over_samples.append(sqrt(mean_squared_error(current_eval_deaths, current_predicted_deaths)))\n",
    "\n",
    "        #calculate mean and confidence interval (95%) based off joint rmse/mae vals\n",
    "        joint_rmse_mean = np.mean(rmse_over_samples)\n",
    "        joint_mae_mean = np.mean(mae_over_samples)\n",
    "   \n",
    "        #calculate mean and confidence interval (95%) based off joint rmse/mae vals\n",
    "        confidence_level = max(0, min(confidence_level, 1)) \n",
    "        \n",
    "        joint_rmse_lower = np.percentile(rmse_over_samples, (1 - confidence_level) * 100 / 2)\n",
    "        joint_rmse_upper = np.percentile(rmse_over_samples, 100 - (1 - confidence_level) * 100 / 2)\n",
    "\n",
    "        joint_mae_lower = np.percentile(mae_over_samples, (1 - confidence_level) * 100 / 2)\n",
    "        joint_mae_upper = np.percentile(mae_over_samples, 100 - (1 - confidence_level) * 100 / 2)\n",
    "\n",
    "        return (joint_rmse_mean, (joint_rmse_lower, joint_rmse_upper)), \\\n",
    "            (joint_mae_mean, (joint_mae_lower, joint_mae_upper))\n",
    "\n",
    "\n",
    "\n",
    "###HELPER function to print results\n",
    "def print_results(metric_name, mean_value, confidence_interval, confidence_level=0.95):\n",
    "    '''Prints results from calculate_metrics'''\n",
    "    print(f\"{metric_name} (Mean, {confidence_level*100:.0f}% CI): {mean_value:.2f}, \"\n",
    "          f\"({confidence_interval[0]:.2f}-{confidence_interval[1]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
