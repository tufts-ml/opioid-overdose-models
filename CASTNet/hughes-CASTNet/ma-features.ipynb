{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dir = '/Users/jyontika/Desktop/'\n",
    "data_dir = os.path.join(user_dir, 'jyontika-MA-data/data/')\n",
    "\n",
    "annual_path = os.path.join(data_dir, 'clean_annual_tract')\n",
    "quarter_path = os.path.join(data_dir, 'clean_quarter_tract')\n",
    "semi_path = os.path.join(data_dir, 'clean_semi_tract')\n",
    "\n",
    "gdf_annual = gpd.read_file(annual_path)\n",
    "gdf_quarter = gpd.read_file(quarter_path)\n",
    "gdf_semi = gpd.read_file(semi_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "castnet_path = os.path.join(user_dir, 'opioid-overdose-models/CASTNet/hughes-CASTNet/Data/MA/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add static feature as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_data = []\n",
    "\n",
    "for geoid in gdf_annual['geoid'].unique():\n",
    "    geoid_df = gdf_annual[gdf_annual['geoid'] == geoid]\n",
    "    \n",
    "    # Extract the lat and lon values \n",
    "    lat = geoid_df['lat'].values[0]\n",
    "    lon = geoid_df['lon'].values[0]\n",
    "    \n",
    "    # append data\n",
    "    geoid_data.append([geoid, lat, lon])\n",
    "\n",
    "\n",
    "static_features = pd.DataFrame(geoid_data)\n",
    "static_features = static_features.transpose()\n",
    "csv_path = os.path.join(castnet_path, 'static_features.csv')\n",
    "static_features.to_csv(csv_path, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1620)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_series = gdf_annual['geoid'].unique()\n",
    "txt_path = os.path.join(castnet_path, 'locations.txt')\n",
    "with open(txt_path, 'w') as file:\n",
    " for index, geoid in enumerate(geoid_series, start=1):\n",
    "        file.write(f\"{index}\\t{geoid}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geoid_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SVI as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVI  dynamic features organization\n",
    "svi_cols = ['geoid', 'year', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc', 'svi_pctile']\n",
    "gdf_subset = gdf_annual[svi_cols]\n",
    "gdf_pivoted = gdf_subset.pivot(index='geoid', columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_geoids = len(gdf_pivoted.index)\n",
    "num_years = len(gdf_pivoted.columns.levels[1])\n",
    "num_values = len(svi_cols[2:])\n",
    "numpy_3d_array = gdf_pivoted.values.reshape(num_unique_geoids, num_years, num_values)\n",
    "svi_path = os.path.join(castnet_path, 'svi.pkl')\n",
    "with open(svi_path, \"wb\") as file:\n",
    "    pickle.dump(numpy_3d_array, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "\n",
    "for loc in range(0, numpy_3d_array.shape[0]):\n",
    "        new_time_idx = 0\n",
    "        for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "            start_idx = i\n",
    "            end_idx = i + 1\n",
    "            if end_idx > numpy_3d_array.shape[1]:\n",
    "                end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "            svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "            new_time_idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "num_time_slots = svi_agg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 22, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_3d_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add overdose as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overdoses pickle file is an array (# days, # locations) shows # of deaths per DAY in every location??\n",
    "\n",
    "overdose_data = gdf_annual.groupby(['year', 'geoid'])['deaths'].sum().reset_index()\n",
    "overdoses_array = overdose_data.pivot_table(index='year', columns='geoid', values='deaths').to_numpy()\n",
    "overdose_path = os.path.join(castnet_path, 'overdose.pkl')\n",
    "with open(overdose_path, \"wb\") as file:\n",
    "    pickle.dump(overdoses_array, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "overdoses_array = np.swapaxes(overdoses_array, 1, 0)\n",
    "\n",
    "overdose_agg = np.zeros(shape=(overdoses_array.shape[0], num_agg_slots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(overdoses_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distances as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "tl_shape_path = os.path.join(data_dir, 'clean_annual_tract/clean_annual_tract.shp')\n",
    "tl_gdf = gpd.read_file(tl_shape_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_locations_df = tl_gdf[['lat', 'lon']].drop_duplicates()\n",
    "unique_locations_df['lat'] = pd.to_numeric(unique_locations_df['lat'])\n",
    "unique_locations_df['lon'] = pd.to_numeric(unique_locations_df['lon'])\n",
    "unique_locations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371.0  # Earth's radius in kilometers\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    dLat = lat2_rad - lat1_rad\n",
    "    dLon = lon2_rad - lon1_rad\n",
    "    a = math.sin(dLat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dLon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = []\n",
    "\n",
    "#check documentation and magnitude on distances\n",
    "\n",
    "for i, row_i in unique_locations_df.iterrows():\n",
    "    #for each row, initialize empty list row_distances to store  distances between row_i and all other locations\n",
    "    row_distances = []\n",
    "    for j, row_j in unique_locations_df.iterrows():\n",
    "        if i != j:  # avoid calculating distance with itself\n",
    "            distance = haversine(row_i['lat'], row_i['lon'], row_j['lat'], row_j['lon'])\n",
    "            row_distances.append(distance)\n",
    "        else:\n",
    "            row_distances.append(0)  # distance with itself is 0\n",
    "    distance_matrix.append(row_distances)\n",
    "\n",
    "distances_csv_path = os.path.join(castnet_path, 'distances.csv')\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix)\n",
    "distance_matrix_df.to_csv(distances_csv_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 1620)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
