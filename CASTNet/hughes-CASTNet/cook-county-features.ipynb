{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve cleaned data frames \n",
    "data_dir = '/Users/jyontika/Desktop/opioid-overdose-models/cook-county/cleaning-cook-county/'\n",
    "gdf_annual = pd.read_csv(f'{data_dir}/cook_county_gdf_year.csv')\n",
    "gdf_quarter = pd.read_csv(f'{data_dir}/cook_county_gdf_quarterly.csv')\n",
    "gdf_semi = pd.read_csv(f'{data_dir}/cook_county_gdf_semiannual.csv')\n",
    "\n",
    "#convert to gpd (was having trouble importing csv as gdf)\n",
    "gdf_annual['geometry'] = gdf_annual['geometry'].apply(wkt.loads)\n",
    "gdf_annual = gpd.GeoDataFrame(gdf_annual, geometry='geometry')\n",
    "gdf_annual.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_annual)\n",
    "\n",
    "gdf_quarter['geometry'] = gdf_quarter['geometry'].apply(wkt.loads)\n",
    "gdf_quarter = gpd.GeoDataFrame(gdf_quarter, geometry='geometry')\n",
    "gdf_quarter.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_quarter)\n",
    "\n",
    "gdf_semi['geometry'] = gdf_semi['geometry'].apply(wkt.loads)\n",
    "gdf_semi = gpd.GeoDataFrame(gdf_semi, geometry='geometry')\n",
    "gdf_semi.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "castnet_path = '/Users/jyontika/Desktop/opioid-overdose-models/CASTNet/hughes-CASTNet/Data/Chicago/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add static feature as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_data = []\n",
    "\n",
    "for geoid in gdf_annual['geoid'].unique():\n",
    "    geoid_df = gdf_annual[gdf_annual['geoid'] == geoid]\n",
    "    \n",
    "    # Extract the lat and lon values \n",
    "    lat = geoid_df['lat'].values[0]\n",
    "    lon = geoid_df['lon'].values[0]\n",
    "    \n",
    "    # append data\n",
    "    geoid_data.append([geoid, lat, lon])\n",
    "\n",
    "\n",
    "static_features = pd.DataFrame(geoid_data)\n",
    "static_features = static_features.transpose()\n",
    "csv_path = os.path.join(castnet_path, 'static_features.csv')\n",
    "static_features.to_csv(csv_path, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1328)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_series = gdf_annual['geoid'].unique()\n",
    "txt_path = os.path.join(castnet_path, 'locations.txt')\n",
    "with open(txt_path, 'w') as file:\n",
    " for index, geoid in enumerate(geoid_series, start=1):\n",
    "        file.write(f\"{index}\\t{geoid}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SVI as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVI  dynamic features organization\n",
    "svi_cols = ['geoid', 'year', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc', 'svi_pctile']\n",
    "gdf_subset = gdf_annual[svi_cols]\n",
    "gdf_pivoted = gdf_subset.pivot(index='geoid', columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_geoids = len(gdf_pivoted.index)\n",
    "num_years = len(gdf_pivoted.columns.levels[1])\n",
    "num_values = len(svi_cols[2:])\n",
    "numpy_3d_array = gdf_pivoted.values.reshape(num_unique_geoids, num_years, num_values)\n",
    "svi_path = os.path.join(castnet_path, 'svi.pkl')\n",
    "with open(svi_path, \"wb\") as file:\n",
    "    pickle.dump(numpy_3d_array, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "\n",
    "for loc in range(0, numpy_3d_array.shape[0]):\n",
    "        new_time_idx = 0\n",
    "        for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "            start_idx = i\n",
    "            end_idx = i + 1\n",
    "            if end_idx > numpy_3d_array.shape[1]:\n",
    "                end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "            svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "            new_time_idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "num_time_slots = svi_agg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 8, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_3d_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add overdose as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overdoses pickle file is an array (# days, # locations) shows # of deaths per DAY in every location??\n",
    "# (8, 1328)\n",
    "\n",
    "overdose_data = gdf_annual.groupby(['year', 'geoid'])['deaths'].sum().reset_index()\n",
    "overdoses_array = overdose_data.pivot_table(index='year', columns='geoid', values='deaths').to_numpy()\n",
    "overdose_path = os.path.join(castnet_path, 'overdose.pkl')\n",
    "with open(overdose_path, \"wb\") as file:\n",
    "    pickle.dump(overdoses_array, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(overdoses_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "overdoses_array = np.swapaxes(overdoses_array, 1, 0)\n",
    "\n",
    "overdose_agg = np.zeros(shape=(overdoses_array.shape[0], num_agg_slots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overdose_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loc in range(0, numpy_3d_array.shape[0]):\n",
    "#         new_time_idx = 0\n",
    "#         for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "#             start_idx = i\n",
    "#             end_idx = i + 1\n",
    "#             if end_idx > numpy_3d_array.shape[1]:\n",
    "#                 end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "#             svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "#             overdose_agg[loc, new_time_idx] = np.sum(overdoses_array[loc, start_idx:end_idx])\n",
    "#             new_time_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distances as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "/Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/cook-county/cleaning-cook-county/files/tl_2021_17_tract/tl_2021_17_tract.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/cook-county/cleaning-cook-county/files/tl_2021_17_tract/tl_2021_17_tract.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/cook-county/cleaning-cook-county/files/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m tl_shape_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mtl_2021_17_tract/tl_2021_17_tract.shp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tl_gdf \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mread_file(tl_shape_path)\n\u001b[1;32m      7\u001b[0m tl_gdf \u001b[39m=\u001b[39m tl_gdf[tl_gdf[\u001b[39m'\u001b[39m\u001b[39mCOUNTYFP\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m031\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/geopandas/io/file.py:281\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         path_or_bytes \u001b[39m=\u001b[39m filename\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_fiona(\n\u001b[1;32m    282\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[39m=\u001b[39;49mbbox, mask\u001b[39m=\u001b[39;49mmask, rows\u001b[39m=\u001b[39;49mrows, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munknown engine \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/geopandas/io/file.py:322\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     reader \u001b[39m=\u001b[39m fiona\u001b[39m.\u001b[39mopen\n\u001b[1;32m    321\u001b[0m \u001b[39mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mwith\u001b[39;00m reader(path_or_bytes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m features:\n\u001b[1;32m    323\u001b[0m         crs \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mcrs_wkt\n\u001b[1;32m    324\u001b[0m         \u001b[39m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/fiona/env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    454\u001b[0m     session \u001b[39m=\u001b[39m DummySession()\n\u001b[1;32m    456\u001b[0m \u001b[39mwith\u001b[39;00m env_ctor(session\u001b[39m=\u001b[39msession):\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/fiona/__init__.py:292\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     path \u001b[39m=\u001b[39m parse_path(fp)\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     colxn \u001b[39m=\u001b[39m Collection(\n\u001b[1;32m    293\u001b[0m         path,\n\u001b[1;32m    294\u001b[0m         mode,\n\u001b[1;32m    295\u001b[0m         driver\u001b[39m=\u001b[39;49mdriver,\n\u001b[1;32m    296\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    297\u001b[0m         layer\u001b[39m=\u001b[39;49mlayer,\n\u001b[1;32m    298\u001b[0m         enabled_drivers\u001b[39m=\u001b[39;49menabled_drivers,\n\u001b[1;32m    299\u001b[0m         allow_unsupported_drivers\u001b[39m=\u001b[39;49mallow_unsupported_drivers,\n\u001b[1;32m    300\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    301\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m     colxn \u001b[39m=\u001b[39m Collection(\n\u001b[1;32m    304\u001b[0m         path,\n\u001b[1;32m    305\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/fiona/collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m Session()\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mstart(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: /Users/jyontika/Desktop/Python/github_hughes/opioid-overdose-models/cook-county/cleaning-cook-county/files/tl_2021_17_tract/tl_2021_17_tract.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "data_dir = '/Users/jyontika/Desktop/opioid-overdose-models/cook-county-data/shapefiles'\n",
    "tl_shape_path = os.path.join(data_dir, 'tl_2021_17_tract/tl_2021_17_tract.shp')\n",
    "tl_gdf = gpd.read_file(tl_shape_path)\n",
    "tl_gdf = tl_gdf[tl_gdf['COUNTYFP']=='031']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoid_to_drop = ['17031990000', '17031381700', '17031980000', '17031980100']\n",
    "tl_gdf = tl_gdf[~tl_gdf['GEOID'].isin(geoid_to_drop)]\n",
    "\n",
    "unique_locations_df = tl_gdf[['lat', 'lon']].drop_duplicates()\n",
    "unique_locations_df['lat'] = pd.to_numeric(unique_locations_df['lat'])\n",
    "unique_locations_df['lon'] = pd.to_numeric(unique_locations_df['lon'])\n",
    "unique_locations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371.0  # Earth's radius in kilometers\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    dLat = lat2_rad - lat1_rad\n",
    "    dLon = lon2_rad - lon1_rad\n",
    "    a = math.sin(dLat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dLon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mention lat and lon points to kyle\n",
    "#find representative center point for each tract \n",
    "#calculates  distance matrix for a set of unique locations based on their latitude and longitude coordinates \n",
    "#average distance between deaths in tract A, deaths in tract B --> grab the pairs and compute haversine \n",
    "#distance between tract A and tractB depends on individual death locations, using average pairwise-distance between deaths in A and deaths in B\n",
    "distance_matrix = []\n",
    "\n",
    "#check documentation and magnitude on distances\n",
    "\n",
    "for i, row_i in unique_locations_df.iterrows():\n",
    "    #for each row, initialize empty list row_distances to store  distances between row_i and all other locations\n",
    "    row_distances = []\n",
    "    for j, row_j in unique_locations_df.iterrows():\n",
    "        if i != j:  # avoid calculating distance with itself\n",
    "            distance = haversine(row_i['lat'], row_i['lon'], row_j['lat'], row_j['lon'])\n",
    "            row_distances.append(distance)\n",
    "        else:\n",
    "            row_distances.append(0)  # distance with itself is 0\n",
    "    distance_matrix.append(row_distances)\n",
    "\n",
    "distances_csv_path = os.path.join(castnet_path, 'distances.csv')\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix)\n",
    "distance_matrix_df.to_csv(distances_csv_path, index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
