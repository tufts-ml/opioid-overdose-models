{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file prepares features to input into CASTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/jyontika/Library/Python/3.9/lib/python/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve cleaned data frames \n",
    "user_dir = '/Users/jyontika/Desktop/'\n",
    "data_dir = os.path.join(user_dir, 'opioid-overdose-models/cook-county/data/')\n",
    "\n",
    "gdf_annual = pd.read_csv(f'{data_dir}/cook_county_gdf_year.csv')\n",
    "gdf_quarter = pd.read_csv(f'{data_dir}/cook_county_gdf_quarterly.csv')\n",
    "gdf_semi = pd.read_csv(f'{data_dir}/cook_county_gdf_semiannual.csv')\n",
    "\n",
    "#convert to gpd (was having trouble importing csv as gdf)\n",
    "gdf_annual['geometry'] = gdf_annual['geometry'].apply(wkt.loads)\n",
    "gdf_annual = gpd.GeoDataFrame(gdf_annual, geometry='geometry')\n",
    "gdf_annual.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_annual)\n",
    "\n",
    "gdf_quarter['geometry'] = gdf_quarter['geometry'].apply(wkt.loads)\n",
    "gdf_quarter = gpd.GeoDataFrame(gdf_quarter, geometry='geometry')\n",
    "gdf_quarter.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_quarter)\n",
    "\n",
    "gdf_semi['geometry'] = gdf_semi['geometry'].apply(wkt.loads)\n",
    "gdf_semi = gpd.GeoDataFrame(gdf_semi, geometry='geometry')\n",
    "gdf_semi.crs = {'init': 'EPSG:4269'}\n",
    "type(gdf_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "castnet_path = os.path.join(user_dir, 'opioid-overdose-models/CASTNet/hughes-CASTNet/Data/Chicago/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add static feature as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_data = []\n",
    "\n",
    "for geoid in gdf_annual['geoid'].unique():\n",
    "    geoid_df = gdf_annual[gdf_annual['geoid'] == geoid]\n",
    "    \n",
    "    # Extract the lat and lon values \n",
    "    lat = geoid_df['lat'].values[0]\n",
    "    lon = geoid_df['lon'].values[0]\n",
    "    \n",
    "    # append data\n",
    "    geoid_data.append([geoid, lat, lon])\n",
    "\n",
    "\n",
    "static_features = pd.DataFrame(geoid_data)\n",
    "static_features = static_features.transpose()\n",
    "csv_path = os.path.join(castnet_path, 'static_features.csv')\n",
    "static_features.to_csv(csv_path, index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1328)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_series = gdf_annual['geoid'].unique()\n",
    "txt_path = os.path.join(castnet_path, 'locations.txt')\n",
    "with open(txt_path, 'w') as file:\n",
    " for index, geoid in enumerate(geoid_series, start=1):\n",
    "        file.write(f\"{index}\\t{geoid}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SVI as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVI  dynamic features organization\n",
    "svi_cols = ['geoid', 'year', 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc', 'svi_pctile']\n",
    "gdf_subset = gdf_annual[svi_cols]\n",
    "gdf_pivoted = gdf_subset.pivot(index='geoid', columns='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_geoids = len(gdf_pivoted.index)\n",
    "num_years = len(gdf_pivoted.columns.levels[1])\n",
    "num_values = len(svi_cols[2:])\n",
    "numpy_3d_array = gdf_pivoted.values.reshape(num_unique_geoids, num_years, num_values)\n",
    "svi_path = os.path.join(castnet_path, 'svi.pkl')\n",
    "with open(svi_path, \"wb\") as file:\n",
    "    pickle.dump(numpy_3d_array, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "\n",
    "for loc in range(0, numpy_3d_array.shape[0]):\n",
    "        new_time_idx = 0\n",
    "        for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "            start_idx = i\n",
    "            end_idx = i + 1\n",
    "            if end_idx > numpy_3d_array.shape[1]:\n",
    "                end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "            svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "            new_time_idx += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "num_time_slots = svi_agg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 8, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_3d_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add overdose as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overdoses pickle file is an array (# days, # locations) shows # of deaths per DAY in every location??\n",
    "# (8, 1328)\n",
    "\n",
    "overdose_data = gdf_annual.groupby(['year', 'geoid'])['deaths'].sum().reset_index()\n",
    "overdoses_array = overdose_data.pivot_table(index='year', columns='geoid', values='deaths').to_numpy()\n",
    "overdose_path = os.path.join(castnet_path, 'overdose.pkl')\n",
    "with open(overdose_path, \"wb\") as file:\n",
    "    pickle.dump(overdoses_array, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(overdoses_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "num_agg_slots = int(math.ceil(numpy_3d_array.shape[1] / float(1)))\n",
    "svi_agg = np.zeros(shape=(numpy_3d_array.shape[0], num_agg_slots, numpy_3d_array.shape[2]))\n",
    "overdoses_array = np.swapaxes(overdoses_array, 1, 0)\n",
    "\n",
    "overdose_agg = np.zeros(shape=(overdoses_array.shape[0], num_agg_slots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overdose_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loc in range(0, numpy_3d_array.shape[0]):\n",
    "#         new_time_idx = 0\n",
    "#         for i in range(0, numpy_3d_array.shape[1], 1):\n",
    "#             start_idx = i\n",
    "#             end_idx = i + 1\n",
    "#             if end_idx > numpy_3d_array.shape[1]:\n",
    "#                 end_idx = numpy_3d_array.shape[1]\n",
    "            \n",
    "#             svi_agg[loc, new_time_idx] = np.sum(numpy_3d_array[loc, start_idx:end_idx], axis=0)\n",
    "#             overdose_agg[loc, new_time_idx] = np.sum(overdoses_array[loc, start_idx:end_idx])\n",
    "#             new_time_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distances as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load census tract shapefile\n",
    "shapefile_dir = os.path.join(data_dir, 'shapefiles')  #shapefiles are on cluster\n",
    "tl_shape_path = os.path.join(shapefile_dir, 'tl_2021_17_tract/tl_2021_17_tract.shp')\n",
    "tl_gdf = gpd.read_file(tl_shape_path)\n",
    "tl_gdf = tl_gdf[tl_gdf['COUNTYFP']=='031']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoid_to_drop = ['17031990000', '17031381700', '17031980000', '17031980100']\n",
    "tl_gdf = tl_gdf[~tl_gdf['GEOID'].isin(geoid_to_drop)]\n",
    "\n",
    "unique_locations_df = tl_gdf[['INTPTLAT', 'INTPTLON']].drop_duplicates()\n",
    "unique_locations_df['lat'] = pd.to_numeric(unique_locations_df['INTPTLAT'])\n",
    "unique_locations_df['lon'] = pd.to_numeric(unique_locations_df['INTPTLON'])\n",
    "unique_locations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the distance between two points using the Haversine formula.\"\"\"\n",
    "    R = 6371.0  # Earth's radius in kilometers\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    dLat = lat2_rad - lat1_rad\n",
    "    dLon = lon2_rad - lon1_rad\n",
    "    a = math.sin(dLat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dLon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mention lat and lon points to kyle\n",
    "#find representative center point for each tract \n",
    "#calculates  distance matrix for a set of unique locations based on their latitude and longitude coordinates \n",
    "#average distance between deaths in tract A, deaths in tract B --> grab the pairs and compute haversine \n",
    "#distance between tract A and tractB depends on individual death locations, using average pairwise-distance between deaths in A and deaths in B\n",
    "distance_matrix = []\n",
    "\n",
    "#check documentation and magnitude on distances\n",
    "\n",
    "for i, row_i in unique_locations_df.iterrows():\n",
    "    #for each row, initialize empty list row_distances to store  distances between row_i and all other locations\n",
    "    row_distances = []\n",
    "    for j, row_j in unique_locations_df.iterrows():\n",
    "        if i != j:  # avoid calculating distance with itself\n",
    "            distance = haversine(row_i['lat'], row_i['lon'], row_j['lat'], row_j['lon'])\n",
    "            row_distances.append(distance)\n",
    "        else:\n",
    "            row_distances.append(0)  # distance with itself is 0\n",
    "    distance_matrix.append(row_distances)\n",
    "\n",
    "distances_csv_path = os.path.join(castnet_path, 'distances.csv')\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix)\n",
    "distance_matrix_df.to_csv(distances_csv_path, index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
