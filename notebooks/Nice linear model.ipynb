{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499d8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 21:22:35.537922: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-28 21:22:35.540984: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-28 21:22:35.680955: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-28 21:22:35.682423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 21:22:48.964320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import IndexSlice as idx\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/opioid-overdose-models/perturbations/'\n",
    "sys.path.append(code_dir)\n",
    "code_dir = '/cluster/home/kheuto01/code/opioid-overdose-models/diff_bpr'\n",
    "sys.path.append(code_dir)\n",
    "from top_k import top_k_idx\n",
    "#from make_datasets import make_data\n",
    "from bpr_model import PerturbedBPRModel\n",
    "\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/opioid-overdose-models/'\n",
    "sys.path.append(code_dir)\n",
    "from zinf_gp.metrics import normcdf, fixed_top_X\n",
    "\n",
    "\n",
    "\n",
    "from perturbations import perturbed\n",
    "from bpr import bpr_variable_k_no_ties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665a03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/cluster/tufts/hugheslab/datasets/NSF_OD/results_20220606_update/clean_quarter_tract/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2743f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_quarterly(multiindexed_gdf, first_year, last_year, time_window, feature_cols, train_shape, pred_lag=1):\n",
    "\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    quarters = []\n",
    "\n",
    "    for eval_year in range(first_year, last_year + 1):\n",
    "        quarters_in_year = multiindexed_gdf[multiindexed_gdf['year']==eval_year].index.unique(level='timestep')\n",
    "        quarters_in_year.sort_values()\n",
    "        train_x_df = multiindexed_gdf.loc[idx[:, min(quarters_in_year) - time_window:max(quarters_in_year) - pred_lag], feature_cols]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for q,quarter in enumerate(quarters_in_year):\n",
    "            \n",
    "            \n",
    "            train_x_vals = train_x_df.values.reshape(train_shape)\n",
    "            \n",
    "            train_y_df = multiindexed_gdf.loc[idx[:,quarter], 'deaths']\n",
    "            train_y_vals = train_y_df.values\n",
    "\n",
    "            xs.append(train_x_vals)\n",
    "            ys.append(train_y_vals)\n",
    "            quarters.append(np.ones_like(train_y_vals)*q)\n",
    "\n",
    "    x_BSTD = np.stack(xs, axis=0)\n",
    "    y_BS = np.stack(ys)\n",
    "\n",
    "    x_BSTD = tf.convert_to_tensor(x_BSTD, dtype=tf.float32)\n",
    "    y_BS = tf.convert_to_tensor(y_BS, dtype=tf.float32)\n",
    "\n",
    "    B, S, T, D = x_BSTD.shape\n",
    "\n",
    "    assert (B == len(range(first_year, last_year + 1))*pred_lag)\n",
    "    assert (S == train_shape[0])\n",
    "    assert (T == time_window)\n",
    "    assert (D == len(feature_cols))\n",
    "\n",
    "    # Reshape the training data to flatten the dimensions\n",
    "    x_BSF_flat = tf.reshape(x_BSTD, (B, S, T * D), )\n",
    "    # add prediction quarter\n",
    "    x_BSF_flat = np.concatenate((x_BSF_flat, np.expand_dims(quarters,axis=-1)),axis=-1)\n",
    "\n",
    "\n",
    "    return x_BSF_flat, y_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "495e0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedBPRModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, perturbed_top_k_func, k=100, **kwargs):\n",
    "        \"\"\"k should match the k baked into the perturbed top_k func.\n",
    "        we need k for when performing exact top k in evaluation step.\"\"\"\n",
    "        super(PerturbedBPRModel, self).__init__(**kwargs)\n",
    "        self.perturbed_top_k_func = perturbed_top_k_func\n",
    "        self.k = k\n",
    "        self.hidden1 = tf.keras.layers.Dense(25, activation='relu')\n",
    "        self.hidden2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        intermediate = self.hidden1(inputs)\n",
    "        intermediate = self.hidden2(intermediate)\n",
    "        \n",
    "        outputs = self.output_layer(intermediate)\n",
    "        # squeeze away feature dimension\n",
    "        outputs = tf.squeeze(outputs, axis=-1)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            top_100_indicators = self.perturbed_top_k_func(y_pred)\n",
    "            true_top_100_val, true_top_100_idx = tf.math.top_k(y, k=self.k)\n",
    "\n",
    "            denominator = tf.reduce_sum(true_top_100_val, axis=-1)\n",
    "            numerator = tf.reduce_sum(top_100_indicators * y, axis=-1)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(numerator, denominator, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)  # Forward pass\n",
    "        # use discrete topk to simulate making a decision\n",
    "        _, pred_100_idx = tf.math.top_k(y_pred, k=self.k)\n",
    "        true_top_100_val, true_top_100_idx = tf.math.top_k(y, k=self.k)\n",
    "\n",
    "        denominator = tf.reduce_sum(true_top_100_val, axis=-1)\n",
    "        numerator = tf.reduce_sum(tf.gather(y, pred_100_idx, batch_dims=-1), axis=-1)\n",
    "\n",
    "        # Compute the loss value\n",
    "        # (the loss function is configured in `compile()`)\n",
    "        self.compiled_loss(numerator, denominator, regularization_losses=self.losses)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8d8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 21:26:31.265949: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-05-28 21:26:31.266002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: p1cmp078.pax.tufts.edu\n",
      "2023-05-28 21:26:31.266012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: p1cmp078.pax.tufts.edu\n",
      "2023-05-28 21:26:31.266167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 515.65.1\n",
      "2023-05-28 21:26:31.266205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 515.65.1\n",
      "2023-05-28 21:26:31.266212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 515.65.1\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "seed = 360\n",
    "time_window = 5*4\n",
    "first_train_eval_year = 2014\n",
    "last_train_eval_year = 2018\n",
    "#batch_dim_size = last_train_eval_year - first_train_eval_year + 1\n",
    "first_validation_year = 2019\n",
    "last_validation_year = 2019\n",
    "first_test_year = 2020\n",
    "last_test_year = 2021\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "timestep_col = 'timestep'\n",
    "geography_col = 'geoid'\n",
    "outcome_col = 'deaths'\n",
    "\n",
    "x_idx_cols = [geography_col, 'lat', 'lon', timestep_col,\n",
    "              'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "              'svi_pctile', 'year',\n",
    "              'neighbor_t', 'deaths']\n",
    "y_idx_cols = [geography_col, timestep_col, outcome_col]\n",
    "#features_only = ['lat', 'lon', timestep_col,\n",
    "#                 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "#                 'svi_pctile',\n",
    "#                 'neighbor_t', 'deaths']\n",
    "features_only = ['deaths']\n",
    "\n",
    "data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "multiindexed_gdf = data_gdf.set_index(['geoid', 'timestep'])\n",
    "multiindexed_gdf['timestep'] = multiindexed_gdf.index.get_level_values('timestep')\n",
    "num_geoids = len(data_gdf['geoid'].unique())\n",
    "\n",
    "train_shape = (num_geoids, time_window, len(features_only))\n",
    "\n",
    "train_x_BSF_flat, train_y_BS = make_data_quarterly(multiindexed_gdf, first_train_eval_year, last_train_eval_year,\n",
    "                                                  time_window, features_only, train_shape, pred_lag=4)\n",
    "\n",
    "valid_x_BSF_flat, valid_y_BS = make_data_quarterly(multiindexed_gdf, first_validation_year, last_validation_year,\n",
    "                                         time_window, features_only, train_shape, pred_lag=4)\n",
    "\n",
    "test_x_BSF_flat, test_y_BS = make_data_quarterly(multiindexed_gdf, first_test_year, last_test_year,\n",
    "                                       time_window, features_only, train_shape, pred_lag=4)\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(train_x_BSF_flat)\n",
    "train_x_BSF_flat = norm_layer(train_x_BSF_flat)\n",
    "valid_x_BSF_flat = norm_layer(valid_x_BSF_flat)\n",
    "test_x_BSF_flat = norm_layer(test_x_BSF_flat)\n",
    "\n",
    "top_100_idx_func = partial(top_k_idx, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b1965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3b2320",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. LinearRegression expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_BSF_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_BS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/ptopk_tf_again/lib/python3.8/site-packages/sklearn/linear_model/_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[1;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[1;32m    657\u001b[0m     X,\n\u001b[1;32m    658\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    662\u001b[0m )\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/ptopk_tf_again/lib/python3.8/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/ptopk_tf_again/lib/python3.8/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/ptopk_tf_again/lib/python3.8/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    921\u001b[0m     _assert_all_finite(\n\u001b[1;32m    922\u001b[0m         array,\n\u001b[1;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. LinearRegression expected <= 2."
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(train_x_BSF_flat, train_y_BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe3f06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 1620, 21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_BSF_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65037615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 1620), dtype=float32, numpy=\n",
       "array([[1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8d3425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (37, 1620, 20)\n",
      "y_train shape: (37, 1620)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random sample data\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "batch_size = 37\n",
    "sequence_length = 1620\n",
    "feature_dim = 20\n",
    "\n",
    "x_train = np.random.randn(batch_size, sequence_length, feature_dim)\n",
    "y_train = np.random.randn(batch_size, sequence_length)\n",
    "\n",
    "# Print the shapes of the generated data\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a7c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1874/1874 [==============================] - 2s 834us/step - loss: 1.1920\n",
      "Epoch 2/10\n",
      "1874/1874 [==============================] - 2s 829us/step - loss: 0.9999\n",
      "Epoch 3/10\n",
      "1874/1874 [==============================] - 2s 825us/step - loss: 1.0005\n",
      "Epoch 4/10\n",
      "1874/1874 [==============================] - 2s 817us/step - loss: 1.0003\n",
      "Epoch 5/10\n",
      "1874/1874 [==============================] - 2s 818us/step - loss: 1.0003\n",
      "Epoch 6/10\n",
      "1874/1874 [==============================] - 2s 814us/step - loss: 1.0004\n",
      "Epoch 7/10\n",
      "1874/1874 [==============================] - 2s 810us/step - loss: 1.0003\n",
      "Epoch 8/10\n",
      "1874/1874 [==============================] - 2s 817us/step - loss: 1.0005\n",
      "Epoch 9/10\n",
      "1874/1874 [==============================] - 2s 815us/step - loss: 1.0003\n",
      "Epoch 10/10\n",
      "1874/1874 [==============================] - 2s 811us/step - loss: 1.0005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af31e7d9580>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random sample data\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "batch_size = 37\n",
    "sequence_length = 1620\n",
    "feature_dim = 20\n",
    "\n",
    "x_train = np.random.randn(batch_size, sequence_length, feature_dim)\n",
    "y_train = np.random.randn(batch_size, sequence_length)\n",
    "\n",
    "# Reshape the data\n",
    "x_train_reshaped = np.reshape(x_train, (batch_size * sequence_length, feature_dim))\n",
    "y_train_reshaped = np.reshape(y_train, (batch_size * sequence_length,))\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(feature_dim,), use_bias=True)  # 1 unit for prediction, 20 input features + bias\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_reshaped, y_train_reshaped, epochs=10, batch_size=32)  # x_train_reshaped: (37 * 1620, 20), y_train_reshaped: (37 * 1620,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9ec7a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59940, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cddc312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23413696,  0.11092259, -0.71984421,  1.35624003, -0.50175704])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0,:5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6cbfb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23413696,  0.11092259, -0.71984421,  1.35624003, -0.50175704])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped[:5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c818a0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 1620, 21), dtype=float32, numpy=\n",
       "array([[[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -1.3416407 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ...,  1.507537  ,\n",
       "         -0.45583406, -1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -1.3416407 ]],\n",
       "\n",
       "       [[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ...,  1.507537  ,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ]],\n",
       "\n",
       "       [[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ...,  1.507537  ,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 ,  2.6376092 , ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [ 2.6732433 , -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406, -0.4472136 ]],\n",
       "\n",
       "       [[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 ,  2.6376092 , ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [ 2.6732433 , -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  0.4472136 ]],\n",
       "\n",
       "       [[-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ],\n",
       "        ...,\n",
       "        [-0.30601794, -0.2999008 ,  2.6376092 , ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ],\n",
       "        [ 2.6732433 , -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ],\n",
       "        [-0.30601794, -0.2999008 , -0.30842423, ..., -0.44685635,\n",
       "         -0.45583406,  1.3416407 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_BSF_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc0445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = np.random.uniform(size=(1620,20))\n",
    "params = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75e6bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multed = thing*params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c75cb872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e617858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.4293966 ,  0.73677363, ..., 11.29133254,\n",
       "        12.34185542, 16.24624365],\n",
       "       [ 0.        ,  0.23881087,  0.39448319, ...,  2.63869672,\n",
       "        16.71658145,  0.73630669],\n",
       "       [ 0.        ,  0.23456134,  0.02409557, ...,  9.59706617,\n",
       "         4.70654871,  5.5814408 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.19647467,  1.86064454, ...,  5.35120039,\n",
       "         2.53750528,  5.58366202],\n",
       "       [ 0.        ,  0.50174264,  0.19006992, ..., 15.61504942,\n",
       "         3.5434477 ,  3.28939923],\n",
       "       [ 0.        ,  0.96734246,  1.20831224, ..., 10.52995387,\n",
       "        12.58761931,  8.56894516]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4db5ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PerturbedBPRModel(top_100_idx_func,name='hi').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd45e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
