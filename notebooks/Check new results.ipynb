{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a1718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 10:19:36.619530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 10:19:36.775878: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-02 10:19:36.781591: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-02 10:19:36.781606: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-02 10:19:36.813113: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-02 10:19:38.197350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 10:19:38.197436: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 10:19:38.197444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import copy\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "code_dir = '/cluster/home/kheuto01/code/zero-inflated-gp/'\n",
    "sys.path.append(code_dir)\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from onoffgpf import OnOffSVGP, OnOffLikelihood\n",
    "\n",
    "import pickle\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import gpflow\n",
    "\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/zero-inflated-gp/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/opioid-overdose-models/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from onoffgpf import OnOffSVGP, OnOffSVGPPoiMC, OnOffLikelihood\n",
    "gpflow.config.default_float()\n",
    "\n",
    "\n",
    "from zinf_gp.metrics import normcdf, fixed_top_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b013bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir='/cluster/tufts/hugheslab/datasets/NSF_OD/results_20220606_update/'\n",
    "\n",
    "log_dir='/cluster/tufts/hugheslab/kheuto01/opioid/logs/converge/'\n",
    "\n",
    "run_template = '{time}_{loc}_{model}_{start_year}_{cov}_{num_inducing}_{lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6343ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['annual', 'quarter','semi']\n",
    "locs = ['tract','town','group']\n",
    "start_years = [ 2000,2010]\n",
    "covs = [ '-auto','-auto-svi','all']\n",
    "models = ['normal', 'poisson']\n",
    "learning_rates = [0.1,0.01,0.001]\n",
    "inducing_points = [400,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998fce66",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/tufts/hugheslab/kheuto01/opioid/logs/larger_zults/quarter_tract_normal_2010_all_200_0.01/stats.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m this_run \u001b[38;5;241m=\u001b[39m run_template\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquarter\u001b[39m\u001b[38;5;124m'\u001b[39m,loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtract\u001b[39m\u001b[38;5;124m'\u001b[39m,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m,start_year\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2010\u001b[39m\u001b[38;5;124m'\u001b[39m,cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m,num_inducing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthis_run\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/tufts/hugheslab/kheuto01/opioid/logs/larger_zults/quarter_tract_normal_2010_all_200_0.01/stats.csv'"
     ]
    }
   ],
   "source": [
    "this_run = run_template.format(time='quarter',loc='tract',model='normal',start_year='2010',cov='all',num_inducing=200,lr=0.01)\n",
    "pd.read_csv(os.path.join(log_dir,this_run,'stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d4d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual_tract_normal_2000_all_400_0.001 2019:0.5149572649572649, 2020:0.5270541082164328\n",
      "annual_tract_normal_2010_all_400_0.001 2019:0.5021367521367521, 2020:0.5450901803607212\n",
      "annual_tract_normal_2000_all_400_0.001 2019:0.5149572649572649, 2020:0.5270541082164328\n",
      "annual_tract_normal_2010_all_400_0.001 2019:0.5021367521367521, 2020:0.5450901803607212\n",
      "annual_tract_normal_2000_all_400_0.001 2019:0.5149572649572649, 2020:0.5270541082164328\n",
      "annual_tract_normal_2010_all_400_0.001 2019:0.5021367521367521, 2020:0.5450901803607212\n",
      "annual_tract_poisson_2000_all_400_0.001 2019:0.5, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_400_0.001 2019:0.5192307692307693, 2020:0.5310621242484971\n",
      "annual_tract_poisson_2000_all_400_0.001 2019:0.5, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_400_0.001 2019:0.5192307692307693, 2020:0.5310621242484971\n",
      "annual_tract_poisson_2000_all_400_0.001 2019:0.5, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_400_0.001 2019:0.5192307692307693, 2020:0.5310621242484971\n",
      "annual_tract_normal_2000_all_200_0.001 2019:0.5064102564102564, 2020:0.5310621242484971\n",
      "annual_tract_normal_2010_all_200_0.001 2019:0.5256410256410257, 2020:0.5310621242484971\n",
      "annual_tract_normal_2000_all_200_0.001 2019:0.5064102564102564, 2020:0.5310621242484971\n",
      "annual_tract_normal_2010_all_200_0.001 2019:0.5256410256410257, 2020:0.5310621242484971\n",
      "annual_tract_normal_2000_all_200_0.001 2019:0.5064102564102564, 2020:0.5310621242484971\n",
      "annual_tract_normal_2010_all_200_0.001 2019:0.5256410256410257, 2020:0.5310621242484971\n",
      "annual_tract_poisson_2000_all_200_0.001 2019:0.5021367521367521, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_200_0.001 2019:0.4615384615384616, 2020:0.5410821643286574\n",
      "annual_tract_poisson_2000_all_200_0.001 2019:0.5021367521367521, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_200_0.001 2019:0.4615384615384616, 2020:0.5410821643286574\n",
      "annual_tract_poisson_2000_all_200_0.001 2019:0.5021367521367521, 2020:0.5230460921843686\n",
      "annual_tract_poisson_2010_all_200_0.001 2019:0.4615384615384616, 2020:0.5410821643286574\n"
     ]
    }
   ],
   "source": [
    "time = 'annual'\n",
    "loc='tract'\n",
    "cov='all'\n",
    "for num_inducing in inducing_points:\n",
    "    for model in models:\n",
    "        for lr in learning_rates:\n",
    "        \n",
    "\n",
    "            for start_year in start_years:    \n",
    "                best_bpr_2019, best_bpr_2020 = -100, -100\n",
    "\n",
    "                for lr in learning_rates:\n",
    "                    this_run = run_template.format(time=time,loc=loc,\n",
    "                                                   model=model,start_year=start_year,\n",
    "                                                   cov=cov,\n",
    "                                                   num_inducing=num_inducing,lr=lr)\n",
    "                    try:\n",
    "                        stats = pd.read_csv(os.path.join(log_dir,this_run,'stats.csv'))\n",
    "                    except(FileNotFoundError):\n",
    "                        print(f'Broke {this_run}')\n",
    "                        continue\n",
    "                    last_row = stats.iloc[-1,:]\n",
    "\n",
    "                    if last_row['bpr_100_0'] > best_bpr_2019:\n",
    "                        best_bpr_2019 = copy.copy(last_row['bpr_100_0'])\n",
    "                    if last_row['bpr_100_1'] > best_bpr_2020:\n",
    "                        best_bpr_2020 = copy.copy(last_row['bpr_100_1'])\n",
    "                print(f'{this_run} 2019:{best_bpr_2019}, 2020:{best_bpr_2020}')\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e16cfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_results = []\n",
    "for num_inducing in inducing_points:\n",
    "    for model in models:\n",
    "        for cov in covs:\n",
    "            for start_year in start_years:\n",
    "                for loc in locs:\n",
    "                    for time in times:\n",
    "                        \n",
    "                        best_bpr_2019, best_bpr_2020 = -100, -100\n",
    "                        \n",
    "                        for lr in learning_rates:\n",
    "                            this_run = run_template.format(time=time,loc=loc,\n",
    "                                                           model=model,start_year=start_year,\n",
    "                                                           cov=cov,\n",
    "                                                           num_inducing=num_inducing,lr=lr)\n",
    "                            try:\n",
    "                                stats = pd.read_csv(os.path.join(log_dir,this_run,'stats.csv'))\n",
    "                            except(FileNotFoundError):\n",
    "                                print(f'Broke {this_run}')\n",
    "                                data = [time, loc, start_year, cov, model, num_inducing, np.NaN, np.NaN]\n",
    "                                basic_results.append(data)\n",
    "                                continue\n",
    "                            last_row = stats.iloc[-1,:]\n",
    "\n",
    "                            if last_row['bpr_100_0'] > best_bpr_2019:\n",
    "                                best_bpr_2019 = copy.copy(last_row['bpr_100_0'])\n",
    "                            if last_row['bpr_100_1'] > best_bpr_2020:\n",
    "                                best_bpr_2020 = copy.copy(last_row['bpr_100_1'])\n",
    "                                \n",
    "                        data = [time, loc, start_year, cov, model, num_inducing, best_bpr_2019, best_bpr_2020]\n",
    "                        basic_results.append(data)\n",
    "pd.DataFrame(basic_results).to_csv('~/result_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ec2f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_325941/2076851216.py\u001b[0m(34)\u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     32 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 34 \u001b[0;31m    \u001b[0mstarting_timestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestep_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'semi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x_timesteps\n",
      "[19.0, 20.0, 19.0, 20.0, 19.0, 20.0, 19.0, 20.0]\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "HNY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 18:52:05.939110: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-16 18:52:05.939153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: p1cmp078.pax.tufts.edu\n",
      "2023-02-16 18:52:05.939161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: p1cmp078.pax.tufts.edu\n",
      "2023-02-16 18:52:05.939306: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2023-02-16 18:52:05.939340: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2023-02-16 18:52:05.939345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2023-02-16 18:52:05.939717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recalc_results = []\n",
    "test_years = 2\n",
    "geography_col='geoid'\n",
    "timestep_col='timestep'\n",
    "outcome_col='deaths'\n",
    "for loc in [ 'tract', 'group','town']:\n",
    "    timesteps_per_year = 4\n",
    "\n",
    "        \n",
    "    file_name = f'clean_quarter_{loc}'\n",
    "    data_path = os.path.join(data_dir, file_name)\n",
    "    \n",
    "    x_idx_cols = [geography_col, 'lat','lon', timestep_col,\n",
    "          'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "          'svi_pctile',\n",
    "          'neighbor_t', 'self_t-1']\n",
    "    y_idx_cols = [geography_col, timestep_col, outcome_col]\n",
    "    features_only = ['lat','lon', timestep_col,\n",
    "                     'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "                     'svi_pctile',\n",
    "                     'neighbor_t', 'self_t-1']\n",
    "\n",
    "    data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "    last_train_year = 2018\n",
    "\n",
    "\n",
    "    test_x = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                      (data_gdf['year'] <= last_train_year+test_years)][x_idx_cols]\n",
    "    test_y = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                      (data_gdf['year'] <= last_train_year+test_years)][y_idx_cols]\n",
    "\n",
    "    import pdb;pdb.set_trace()\n",
    "    starting_timestep = int(test_x[timestep_col].min())\n",
    "\n",
    "    for time in ['semi', 'annual']:\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        for start_year in start_years:\n",
    "            print('HNY')\n",
    "            \n",
    "            for num_inducing in inducing_points:\n",
    "                for model in models:\n",
    "                    for cov in covs:\n",
    "                    \n",
    "                        best_bpr_2019, best_bpr_2020 = -100, -100\n",
    "                        \n",
    "                        for lr in learning_rates:\n",
    "                            this_run = run_template.format(time=time,loc=loc,\n",
    "                                                           model=model,start_year=start_year,\n",
    "                                                           cov=cov,\n",
    "                                                           num_inducing=num_inducing,lr=lr)\n",
    "                            try:\n",
    "                                with open(os.path.join(log_dir,this_run,'model.mod'),'rb') as f:\n",
    "                                        predictor = pickle.load(f)\n",
    "                                    \n",
    "                            except(FileNotFoundError):\n",
    "                                print(f'Broke {this_run}')\n",
    "                                data = [time, loc, start_year, cov, model, num_inducing, np.NaN, np.NaN]\n",
    "                                result.append(data)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            xtops = []\n",
    "                            for year in range(test_years):\n",
    "                                xtop_year = []\n",
    "                                for timestep in range(starting_timestep+year*timesteps_per_year, starting_timestep+year*timesteps_per_year+timesteps_per_year):\n",
    "                                    test_x_time = test_x[test_x[timestep_col] == timestep]\n",
    "                                    test_y_time = test_y[test_y[timestep_col] == timestep]\n",
    "                                    _, _, _, fmean, fvar, gmean, gvar, _, _ = predictor.build_predict(test_x_time.loc[:, features_only].values)\n",
    "                                    g_cond = tf.math.softplus(fmean * normcdf(gmean) + 2).numpy()\n",
    "                                    pred_df = pd.Series(g_cond.squeeze(), index=test_y_time[geography_col])\n",
    "\n",
    "                                    xtop_year.append(fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100))\n",
    "                                xtops.append(xtop_year)\n",
    "                            curr_results = {} \n",
    "                            for y, xtop in enumerate( xtops):\n",
    "                                curr_results[f'bpr_100_{y}'] = copy.deepcopy(np.mean([thing[3] for thing in xtop]))\n",
    "                                \n",
    "                            if curr_results[f'bpr_100_0'] > best_bpr_2019:\n",
    "                                best_bpr_2019 = copy.copy(curr_results[f'bpr_100_0'])\n",
    "                            if curr_results[f'bpr_100_1'] > best_bpr_2020:\n",
    "                                best_bpr_2020 = copy.copy(curr_results[f'bpr_100_1'])\n",
    "                                \n",
    "\n",
    "                        data = [time, loc, start_year, cov, model, num_inducing, best_bpr_2019, best_bpr_2020]\n",
    "                        recalc_results.append(data)\n",
    "    \n",
    "        pd.DataFrame(recalc_results).to_csv(f'~/recalc_results_sheet_{loc}_{time}.csv')\n",
    "pd.DataFrame(recalc_results).to_csv('~/recalc_results_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0024215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cluster/tufts/hugheslab/kheuto01/opioid/logs/new_big_run/annual_tract_poisson_2010_all_400_0.001'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(log_dir,this_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b917f3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.6M\r\n",
      "drwxrws--- 2 kheuto01 hugheslab 4.0K Feb 14 18:00 ..\r\n",
      "drwxrws--- 2 kheuto01 hugheslab 4.0K Feb 14 19:07 .\r\n",
      "-rw-rw---- 1 kheuto01 hugheslab 2.2K Feb 14 19:14 stats.csv\r\n",
      "-rw-rw---- 1 kheuto01 hugheslab 2.6M Feb 14 19:14 model.mod\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth /cluster/tufts/hugheslab/kheuto01/opioid/logs/new_big_run/annual_tract_poisson_2010_all_400_0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "553b0d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>timestep</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>25001010100</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>25001010100</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>25001010100</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>25001010100</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>25001010100</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142551</th>\n",
       "      <td>25027761402</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142552</th>\n",
       "      <td>25027761402</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142553</th>\n",
       "      <td>25027761402</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142554</th>\n",
       "      <td>25027761402</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142555</th>\n",
       "      <td>25027761402</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              geoid  timestep  deaths\n",
       "76      25001010100      76.0     0.0\n",
       "77      25001010100      77.0     0.0\n",
       "78      25001010100      78.0     1.0\n",
       "79      25001010100      79.0     0.0\n",
       "80      25001010100      80.0     0.0\n",
       "...             ...       ...     ...\n",
       "142551  25027761402      79.0     0.0\n",
       "142552  25027761402      80.0     0.0\n",
       "142553  25027761402      81.0     0.0\n",
       "142554  25027761402      82.0     0.0\n",
       "142555  25027761402      83.0     0.0\n",
       "\n",
       "[12960 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6be34d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_timestep = int(test_x[timestep_col].min())\n",
    "\n",
    "\n",
    "xtops = []\n",
    "for year in range(test_years):\n",
    "    xtop_year = []\n",
    "    for timestep in range(starting_timestep+year*timesteps_per_year, starting_timestep+year*timesteps_per_year+timesteps_per_year):\n",
    "        test_x_time = test_x[test_x[timestep_col] == timestep]\n",
    "        test_y_time = test_y[test_y[timestep_col] == timestep]\n",
    "        _, _, _, fmean, fvar, gmean, gvar, _, _ = predictor.build_predict(test_x_time.loc[:, features_only].values)\n",
    "        g_cond = tf.math.softplus(fmean * normcdf(gmean) + 2).numpy()\n",
    "        pred_df = pd.Series(g_cond.squeeze(), index=test_y_time[geography_col])\n",
    "\n",
    "        xtop_year.append(fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100))\n",
    "    xtops.append(xtop_year)\n",
    "curr_results = {} \n",
    "for y, xtop in enumerate( xtops):\n",
    "    curr_results[f'bpr_100_{y}'] = copy.deepcopy(np.mean([thing[3] for thing in xtop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6905833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bpr_100_0': 0.28976780537671637, 'bpr_100_1': 0.3114375293078327}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "388c8c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semi</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto</td>\n",
       "      <td>normal</td>\n",
       "      <td>200</td>\n",
       "      <td>0.690849</td>\n",
       "      <td>0.719448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>semi</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto</td>\n",
       "      <td>normal</td>\n",
       "      <td>200</td>\n",
       "      <td>0.718195</td>\n",
       "      <td>0.734243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>semi</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto-svi</td>\n",
       "      <td>normal</td>\n",
       "      <td>200</td>\n",
       "      <td>0.668744</td>\n",
       "      <td>0.688174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semi</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto-svi</td>\n",
       "      <td>normal</td>\n",
       "      <td>200</td>\n",
       "      <td>0.674319</td>\n",
       "      <td>0.688174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>semi</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>all</td>\n",
       "      <td>normal</td>\n",
       "      <td>200</td>\n",
       "      <td>0.875090</td>\n",
       "      <td>0.876031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>annual</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>all</td>\n",
       "      <td>normal</td>\n",
       "      <td>400</td>\n",
       "      <td>0.844269</td>\n",
       "      <td>0.842457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>annual</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>all</td>\n",
       "      <td>normal</td>\n",
       "      <td>400</td>\n",
       "      <td>0.844269</td>\n",
       "      <td>0.842457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>annual</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto</td>\n",
       "      <td>poisson</td>\n",
       "      <td>400</td>\n",
       "      <td>0.823560</td>\n",
       "      <td>0.823215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>annual</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto</td>\n",
       "      <td>poisson</td>\n",
       "      <td>400</td>\n",
       "      <td>0.882279</td>\n",
       "      <td>0.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>annual</td>\n",
       "      <td>town</td>\n",
       "      <td>2000</td>\n",
       "      <td>-auto-svi</td>\n",
       "      <td>poisson</td>\n",
       "      <td>400</td>\n",
       "      <td>0.557634</td>\n",
       "      <td>0.573285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2          3        4    5         6         7\n",
       "0     semi  town  2000      -auto   normal  200  0.690849  0.719448\n",
       "1     semi  town  2000      -auto   normal  200  0.718195  0.734243\n",
       "2     semi  town  2000  -auto-svi   normal  200  0.668744  0.688174\n",
       "3     semi  town  2000  -auto-svi   normal  200  0.674319  0.688174\n",
       "4     semi  town  2000        all   normal  200  0.875090  0.876031\n",
       "..     ...   ...   ...        ...      ...  ...       ...       ...\n",
       "64  annual  town  2000        all   normal  400  0.844269  0.842457\n",
       "65  annual  town  2000        all   normal  400  0.844269  0.842457\n",
       "66  annual  town  2000      -auto  poisson  400  0.823560  0.823215\n",
       "67  annual  town  2000      -auto  poisson  400  0.882279  0.893014\n",
       "68  annual  town  2000  -auto-svi  poisson  400  0.557634  0.573285\n",
       "\n",
       "[69 rows x 8 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(recalc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac47d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n",
      "HNY\n"
     ]
    }
   ],
   "source": [
    "recalc_results = []\n",
    "test_years = 2\n",
    "geography_col='geoid'\n",
    "timestep_col='timestep'\n",
    "outcome_col='deaths'\n",
    "\n",
    "town_map = pd.read_csv(os.path.join(data_dir,'town_tract_map.csv'), dtype=str)\n",
    "group_map = gpd.read_file(os.path.join(data_dir, 'tract_group_map'), dtype=str)\n",
    "\n",
    "\n",
    "# test y always comes from quarterly tract\n",
    "y_timesteps_per_year = 4\n",
    "file_name = f'clean_quarter_tract'\n",
    "data_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "x_idx_cols = [geography_col, 'lat','lon', timestep_col,\n",
    "      'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "      'svi_pctile',\n",
    "      'neighbor_t', 'self_t-1']\n",
    "y_idx_cols = [geography_col, timestep_col, outcome_col]\n",
    "features_only = ['lat','lon', timestep_col,\n",
    "                 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "                 'svi_pctile',\n",
    "                 'neighbor_t', 'self_t-1']\n",
    "\n",
    "data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "last_train_year = 2018\n",
    "\n",
    "\n",
    "test_y = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                  (data_gdf['year'] <= last_train_year+test_years)][y_idx_cols]\n",
    "starting_y_timestep = int(test_y[timestep_col].min())\n",
    "\n",
    "sorted_y_timesteps = test_y[timestep_col].unique()\n",
    "sorted_y_timesteps.sort()\n",
    "\n",
    "for loc in locs:\n",
    "    for time in times:\n",
    "        x_timesteps_per_year = {'quarter':4, 'semi':2,'annual':1}[time]\n",
    "        \n",
    "        file_name = f'clean_{time}_{loc}'\n",
    "        data_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "        data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "        test_x = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                          (data_gdf['year'] <= last_train_year+test_years)][x_idx_cols]\n",
    "        \n",
    "        starting_x_timestep = int(test_x[timestep_col].min())\n",
    "        \n",
    "        test_timesteps_per_year = max(y_timesteps_per_year, x_timesteps_per_year)\n",
    "        test_timesteps = test_timesteps_per_year*test_years\n",
    "        \n",
    "        x_repeats = int(test_timesteps_per_year/x_timesteps_per_year)\n",
    "        y_repeats = int(test_timesteps_per_year/y_timesteps_per_year)\n",
    "        \n",
    "        sorted_x_timesteps = test_x[timestep_col].unique()\n",
    "        sorted_x_timesteps.sort()\n",
    "        \n",
    "        x_timesteps = [timestep  for timestep in sorted_x_timesteps for _ in range(x_repeats)]\n",
    "        \n",
    "        y_timesteps = [timestep  for timestep in sorted_y_timesteps for _ in range(y_repeats)]\n",
    "\n",
    "        for start_year in start_years:\n",
    "            print('HNY')\n",
    "            \n",
    "            for num_inducing in inducing_points:\n",
    "                for model in models:\n",
    "                    for cov in covs:\n",
    "                    \n",
    "                        best_elbo = -np.inf\n",
    "                        bpr_2019 = np.NaN\n",
    "                        bpr_2020 = np.NaN\n",
    "                        \n",
    "                        for lr in learning_rates:\n",
    "                            #print(lr)\n",
    "                            this_run = run_template.format(time=time,loc=loc,\n",
    "                                                           model=model,start_year=start_year,\n",
    "                                                           cov=cov,\n",
    "                                                           num_inducing=num_inducing,lr=lr)\n",
    "                            try:\n",
    "                                with open(os.path.join(log_dir,this_run,'model.mod'),'rb') as f:\n",
    "                                        predictor = pickle.load(f)\n",
    "                                with open(os.path.join(log_dir,this_run,'stats.csv'),'rb') as f:\n",
    "                                    stats = pd.read_csv(f)\n",
    "                                    elbo = stats.iloc[-1,:][['elbo']].values[0]\n",
    "                                    \n",
    "                            except(FileNotFoundError):\n",
    "                                #print(f'Broke {this_run}')\n",
    "                                data = [time, loc, start_year, cov, model, num_inducing, np.NaN, np.NaN]\n",
    "                                recalc_results.append(data)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            xtops = []\n",
    "                            for year in range(test_years):\n",
    "                                xtop_year = []\n",
    "                                max_timesteps = max(x_timesteps_per_year, y_timesteps_per_year)\n",
    "                                for x_time,y_time in zip(x_timesteps[year*max_timesteps:(year+1)*max_timesteps],\n",
    "                                                         y_timesteps[year*max_timesteps:(year+1)*max_timesteps]):\n",
    "                                    test_x_time = test_x[test_x[timestep_col] == x_time]\n",
    "                                    test_y_time = test_y[test_y[timestep_col] == y_time]\n",
    "                                    _, _, _, fmean, fvar, gmean, gvar, _, _ = predictor.build_predict(test_x_time.loc[:, features_only].values)\n",
    "                                    g_cond = tf.math.softplus(fmean * normcdf(gmean)).numpy()\n",
    "                                    pred_df = pd.Series(g_cond.squeeze(), index=test_x_time[geography_col])\n",
    "                                    \n",
    "                                    if loc == 'town':\n",
    "                                        merged_to_map = town_map.merge(pred_df.rename(outcome_col), right_index=True,left_on='parent_town', how='right')\n",
    "                                        averaged_over_duplicates = merged_to_map.groupby('child_tracts').mean()[outcome_col]\n",
    "                                        y_index = test_y[geography_col].unique()\n",
    "                                        pred_df = pd.Series(index=y_index,dtype='float64')\n",
    "                                        pred_df.update(averaged_over_duplicates)\n",
    "                                        # not all tracts get mapped from towns\n",
    "                                        pred_df=pred_df.fillna(0)\n",
    "                                    elif loc == 'group':\n",
    "                                        merged_to_map = group_map.merge(pred_df.rename(outcome_col), right_index=True,left_on='grouping', how='right')\n",
    "                                        averaged_over_duplicates = merged_to_map.groupby('geoid').mean()[outcome_col]\n",
    "                                        y_index = test_y[geography_col].unique()\n",
    "                                        pred_df = pd.Series(index=y_index,dtype='float64')\n",
    "                                        pred_df.update(averaged_over_duplicates)\n",
    "                                        \n",
    "                                    xtop_year.append(fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100))\n",
    "                                xtops.append(xtop_year)\n",
    "                            curr_results = {} \n",
    "                            for y, xtop in enumerate( xtops):\n",
    "                                curr_results[f'bpr_100_{y}'] = copy.deepcopy(np.mean([thing[3] for thing in xtop]))\n",
    "                                \n",
    "                            bpr_2019 = copy.copy(curr_results[f'bpr_100_0'])\n",
    "                            bpr_2020 = copy.copy(curr_results[f'bpr_100_1'])\n",
    "                            #print(bpr_2019)\n",
    "                                \n",
    "                            if elbo > best_elbo:\n",
    "                                best_bpr_2019 = copy.copy(bpr_2019)\n",
    "                                best_bpr_2020 = copy.copy(bpr_2020)\n",
    "                                best_elbo = elbo\n",
    "                                \n",
    "\n",
    "                        data = [time, loc, start_year, cov, model, num_inducing, best_bpr_2019, best_bpr_2020]\n",
    "                        recalc_results.append(data)\n",
    "\n",
    "        pd.DataFrame(recalc_results).to_csv(f'~/recalc_conv_all_q_results_sheet_{loc}_{time}.csv')\n",
    "pd.DataFrame(recalc_results).to_csv('~/recalc_conv_all_q_results_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c534e8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['annual',\n",
       "  'tract',\n",
       "  2010,\n",
       "  '-auto',\n",
       "  'normal',\n",
       "  400,\n",
       "  0.06432748538011698,\n",
       "  0.048913043478260865],\n",
       " ['annual',\n",
       "  'tract',\n",
       "  2010,\n",
       "  '-auto',\n",
       "  'poisson',\n",
       "  400,\n",
       "  0.23391812865497075,\n",
       "  0.2989130434782608],\n",
       " ['annual',\n",
       "  'tract',\n",
       "  2000,\n",
       "  '-auto',\n",
       "  'normal',\n",
       "  400,\n",
       "  0.4152046783625731,\n",
       "  0.22826086956521732],\n",
       " ['annual',\n",
       "  'tract',\n",
       "  2000,\n",
       "  '-auto',\n",
       "  'poisson',\n",
       "  400,\n",
       "  0.2456140350877192,\n",
       "  0.19565217391304346],\n",
       " ['semi',\n",
       "  'tract',\n",
       "  2010,\n",
       "  '-auto',\n",
       "  'normal',\n",
       "  400,\n",
       "  0.2783724848845277,\n",
       "  0.2193775589783584],\n",
       " ['semi',\n",
       "  'tract',\n",
       "  2010,\n",
       "  '-auto',\n",
       "  'poisson',\n",
       "  400,\n",
       "  0.24680344930121906,\n",
       "  0.16529781633846752],\n",
       " ['semi',\n",
       "  'tract',\n",
       "  2000,\n",
       "  '-auto',\n",
       "  'normal',\n",
       "  400,\n",
       "  0.3049856279115869,\n",
       "  0.30131360889062186],\n",
       " ['semi',\n",
       "  'tract',\n",
       "  2000,\n",
       "  '-auto',\n",
       "  'poisson',\n",
       "  400,\n",
       "  0.32262860541183463,\n",
       "  0.2827671085981672]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "624eb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001010100    42.085380\n",
       "25001010206     2.357825\n",
       "25001010208     2.357825\n",
       "25001010304    42.516030\n",
       "25001010306    17.022105\n",
       "                 ...    \n",
       "25027761100    42.931377\n",
       "25027761200    41.283749\n",
       "25027761300    41.381646\n",
       "25027761401    41.406066\n",
       "25027761402    41.406066\n",
       "Length: 1620, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_zult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a85cf14",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpred_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeaths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m(town_map, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_town\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/miniconda3/envs/modern_zigp/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'merge'"
     ]
    }
   ],
   "source": [
    "pred_df.rename('deaths').merge(town_map, left_index=True,right_on='parent_town')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9776a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254191/3016699809.py:4: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  mapped_preds = pd.Series(index=y_index)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db491a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001012102    0.0\n",
       "25001012200    0.0\n",
       "25001014700    0.0\n",
       "25001990000    0.0\n",
       "25005640100    0.0\n",
       "25005653301    0.0\n",
       "25005990000    0.0\n",
       "25009204400    0.0\n",
       "25009223100    0.0\n",
       "25009268400    0.0\n",
       "25019950307    0.0\n",
       "25019990000    0.0\n",
       "25021417400    0.0\n",
       "25021417501    0.0\n",
       "25021417602    0.0\n",
       "25021417801    0.0\n",
       "25021422800    0.0\n",
       "25021423102    0.0\n",
       "25023500101    0.0\n",
       "25023500103    0.0\n",
       "25023505103    0.0\n",
       "25023505104    0.0\n",
       "25023505201    0.0\n",
       "25023506205    0.0\n",
       "25023530200    0.0\n",
       "25023530300    0.0\n",
       "25023530700    0.0\n",
       "25023530802    0.0\n",
       "25023545400    0.0\n",
       "25023990003    0.0\n",
       "25025060604    0.0\n",
       "25025980101    0.0\n",
       "25025981202    0.0\n",
       "25025990101    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_preds[[val for val in mapped_preds.index if val not in averaged_over_duplicates.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80dceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "town_map = pd.read_csv(os.path.join(data_dir,'town_tract_map.csv'),dtype=str)\n",
    "group_map = gpd.read_file(os.path.join(data_dir, 'tract_group_map'),dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a54267d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e105d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(111.0, 0.3508771929824561, 111.0, 0.35087719298245623),\n",
       "  (132.0, 0.2542372881355932, 132.0, 0.2542372881355933),\n",
       "  (143.0, 0.23529411764705882, 143.0, 0.23529411764705885),\n",
       "  (141.0, 0.22950819672131148, 141.0, 0.22950819672131137)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da111616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
