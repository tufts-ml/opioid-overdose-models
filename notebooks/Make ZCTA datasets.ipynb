{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b320de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 15:05:19.590613: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 15:05:19.750002: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-04 15:05:19.755888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 15:05:19.755903: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 15:05:19.787654: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-04 15:05:21.002295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 15:05:21.002390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 15:05:21.002398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run a zero-inflated GP on opioid data\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import copy\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "code_dir = '/cluster/home/kheuto01/code/zero-inflated-gp/'\n",
    "sys.path.append(code_dir)\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from onoffgpf import OnOffSVGP, OnOffLikelihood\n",
    "\n",
    "import pickle\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4914c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    https://stackoverflow.com/a/4913653/1748679\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b815c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/cluster/tufts/hugheslab/datasets/NSF_OD/'\n",
    "result_dir = os.path.join(data_dir, 'results_20220606_update')\n",
    "mass_shapefile = os.path.join(data_dir,'shapefiles','MA_2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13c8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_file = os.path.join(result_dir, 'svi_month_zcta')\n",
    "svi_gdf = gpd.read_file(svi_file)\n",
    "# Call it \"grid_squar\" because geopandas only supports len 10 columns\n",
    "svi_gdf = svi_gdf.rename(columns={'INTPTLAT20': 'lat', 'INTPTLON20': 'lon', 'GEOID20': 'geoid'})\n",
    "# Make lat and lon floats\n",
    "svi_gdf.loc[:, 'lat'] = svi_gdf.lat.astype(float)\n",
    "svi_gdf.loc[:, 'lon'] = svi_gdf.lon.astype(float)\n",
    "deaths_gdf = svi_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee79039",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Used when we just need the unique tracts and their locations\n",
    "    just_grid = deaths_gdf.loc[\n",
    "        (deaths_gdf['year'] == 2000) & (deaths_gdf['month'] == 1), ['geoid', 'geometry', 'lat', 'lon']]\n",
    "\n",
    "    # Calculate each squares neighbors\n",
    "    neighbors = {}\n",
    "    for _, row in just_grid.iterrows():\n",
    "        just_grid.loc[:, 'haversine'] = just_grid.apply(lambda x: haversine(row['lon'], row['lat'],\n",
    "                                                                            x['lon'], x['lat']),\n",
    "                                                        axis=1)\n",
    "        matching_neighbors = just_grid[just_grid['haversine'] < 8]['geoid'].values\n",
    "        neighbors[row['geoid']] = matching_neighbors\n",
    "\n",
    "    tracts = deaths_gdf['geoid'].unique()\n",
    "    min_year = deaths_gdf.year.min()\n",
    "    max_year = deaths_gdf.year.max()\n",
    "    deaths_gdf = deaths_gdf.set_index(['geoid', 'year', 'month']).sort_index()\n",
    "\n",
    "    month_since_2000 = 0\n",
    "    season_since_2000 = 0\n",
    "    qtr_since_2000 = 0\n",
    "    year_since_2000 = 0\n",
    "    for year in range(min_year, max_year + 1):\n",
    "        for month in range(1, 12 + 1):\n",
    "\n",
    "            if month in [1, 2, 3, 4, 5, 6]:\n",
    "                season = 'jan-jun'\n",
    "            else:\n",
    "                season = 'jul-dec'\n",
    "\n",
    "            if month <= 3:\n",
    "                qtr = 1\n",
    "            elif month <= 6:\n",
    "                qtr = 2\n",
    "            elif month <= 9:\n",
    "                qtr = 3\n",
    "            else:\n",
    "                qtr = 4\n",
    "\n",
    "            deaths_gdf.loc[idx[:, year, month], 'month_since_2000'] = month_since_2000\n",
    "            deaths_gdf.loc[idx[:, year, month], 'season'] = season\n",
    "            deaths_gdf.loc[idx[:, year, month], 'season_since_2000'] = season_since_2000\n",
    "            deaths_gdf.loc[idx[:, year, month], 'quarter'] = qtr\n",
    "            deaths_gdf.loc[idx[:, year, month], 'qtr_since_2000'] = qtr_since_2000\n",
    "            deaths_gdf.loc[idx[:, year, month], 'year_since_2000'] = year_since_2000\n",
    "\n",
    "            month_since_2000 += 1\n",
    "\n",
    "            if month in [6, 12]:\n",
    "                season_since_2000 += 1\n",
    "\n",
    "            if month in [3, 6, 9, 12]:\n",
    "                qtr_since_2000 += 1\n",
    "\n",
    "            if month == 12:\n",
    "                year_since_2000 += 1\n",
    "\n",
    "    deaths_gdf = deaths_gdf.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a92bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_gdf = deaths_gdf.set_index(['geoid', 'year', 'quarter']).sort_index()\n",
    "cleaned_gdf.loc[idx[:, :, :], 'self_t-1'] = cleaned_gdf.loc[idx[:, :, :], 'deaths'].shift(1, fill_value=0)\n",
    "unduped_gdf = cleaned_gdf[~cleaned_gdf.index.duplicated(keep='first')]\n",
    "summed_deaths = cleaned_gdf.groupby(level=[0,1,2]).sum()[['deaths']]\n",
    "summed_deaths = summed_deaths.merge(unduped_gdf, how='left', left_index=True, right_index=True,suffixes=[None,'_garbage'])\n",
    "summed_deaths = summed_deaths.drop('deaths_garbage',axis=1)\n",
    "cleaned_gdf = summed_deaths\n",
    "for tract in tracts:\n",
    "    cleaned_gdf.loc[idx[tract, :, :], 'neighbor_t-1'] = \\\n",
    "        cleaned_gdf.loc[idx[neighbors[tract], :, :], 'self_t-1'].groupby(level=['year', 'quarter']).mean().shift(1,\n",
    "                                                                                                                fill_value=0).values\n",
    "\n",
    "timestep = 0\n",
    "\n",
    "for year in range(min_year, max_year + 1):\n",
    "    for quarter in range(1, 5):\n",
    "        cleaned_gdf.loc[idx[:, year, quarter], 'timestep'] = timestep\n",
    "        timestep += 1\n",
    "\n",
    "cleaned_gdf = cleaned_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2994d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136488/311410610.py:2: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)\n"
     ]
    }
   ],
   "source": [
    "svi_out_file = os.path.join(result_dir, 'clean_quarter_zcta')\n",
    "gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4718d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_gdf = deaths_gdf.set_index(['geoid', 'year', 'season']).sort_index()\n",
    "cleaned_gdf.loc[idx[:, :, :], 'self_t-1'] = cleaned_gdf.loc[idx[:, :, :], 'deaths'].shift(1, fill_value=0)\n",
    "unduped_gdf = cleaned_gdf[~cleaned_gdf.index.duplicated(keep='first')]\n",
    "summed_deaths = cleaned_gdf.groupby(level=[0,1,2]).sum()[['deaths']]\n",
    "summed_deaths = summed_deaths.merge(unduped_gdf, how='left', left_index=True, right_index=True,suffixes=[None,'_garbage'])\n",
    "summed_deaths = summed_deaths.drop('deaths_garbage',axis=1)\n",
    "cleaned_gdf = summed_deaths\n",
    "for tract in tracts:\n",
    "    cleaned_gdf.loc[idx[tract, :, :], 'neighbor_t-1'] = \\\n",
    "        cleaned_gdf.loc[idx[neighbors[tract], :, :], 'self_t-1'].groupby(level=['year', 'season']).mean().shift(1,\n",
    "                                                                                                                fill_value=0).values\n",
    "\n",
    "timestep = 0\n",
    "\n",
    "for year in range(min_year, max_year + 1):\n",
    "    for season in ['jan-jun', 'jul-dec']:\n",
    "        cleaned_gdf.loc[idx[:, year, season], 'timestep'] = timestep\n",
    "        timestep += 1\n",
    "\n",
    "cleaned_gdf = cleaned_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e4f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136488/1799216974.py:2: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)\n"
     ]
    }
   ],
   "source": [
    "svi_out_file = os.path.join(result_dir, 'clean_semi_zcta')\n",
    "gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20604b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_gdf = deaths_gdf.set_index(['geoid', 'year']).sort_index()\n",
    "cleaned_gdf.loc[idx[:, :, :], 'self_t-1'] = cleaned_gdf.loc[idx[:, :], 'deaths'].shift(1, fill_value=0)\n",
    "unduped_gdf = cleaned_gdf[~cleaned_gdf.index.duplicated(keep='first')]\n",
    "summed_deaths = cleaned_gdf.groupby(level=[0,1]).sum()[['deaths']]\n",
    "summed_deaths = summed_deaths.merge(unduped_gdf, how='left', left_index=True, right_index=True,suffixes=[None,'_garbage'])\n",
    "summed_deaths = summed_deaths.drop('deaths_garbage',axis=1)\n",
    "cleaned_gdf = summed_deaths\n",
    "for tract in tracts:\n",
    "    cleaned_gdf.loc[idx[tract, :], 'neighbor_t-1'] = \\\n",
    "        cleaned_gdf.loc[idx[neighbors[tract], :], 'self_t-1'].groupby(level=['year']).mean().shift(1, fill_value=0).values\n",
    "\n",
    "timestep = 0\n",
    "\n",
    "for year in range(min_year, max_year + 1):\n",
    "        cleaned_gdf.loc[idx[:, year], 'timestep'] = timestep\n",
    "        timestep += 1\n",
    "\n",
    "cleaned_gdf = cleaned_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b3f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136488/4117635226.py:2: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)\n"
     ]
    }
   ],
   "source": [
    "svi_out_file = os.path.join(result_dir, 'clean_annual_zcta')\n",
    "gpd.GeoDataFrame(cleaned_gdf).to_file(svi_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ef148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
