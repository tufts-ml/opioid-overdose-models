{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65510db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 13:42:35.916269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 13:42:36.391741: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-28 13:42:36.643714: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:42:36.643735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 13:42:36.690234: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-28 13:42:41.034322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:42:41.034847: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:42:41.034857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import copy\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "code_dir = '/cluster/home/kheuto01/code/zero-inflated-gp/'\n",
    "sys.path.append(code_dir)\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from onoffgpf import OnOffSVGP, OnOffLikelihood\n",
    "\n",
    "import pickle\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import gpflow\n",
    "\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/zero-inflated-gp/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "code_dir = '/cluster/home/kheuto01/code/opioid-overdose-models/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from onoffgpf import OnOffSVGP, OnOffSVGPPoiMC, OnOffLikelihood\n",
    "gpflow.config.default_float()\n",
    "\n",
    "\n",
    "from zinf_gp.metrics import normcdf, fixed_top_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f79f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir='/cluster/tufts/hugheslab/datasets/NSF_OD/results_20220606_update/'\n",
    "\n",
    "log_dir='/cluster/tufts/hugheslab/kheuto01/opioid/logs/fix_two/'\n",
    "\n",
    "run_template = '{time}_{loc}_{model}_{start_year}_{cov}_{num_inducing}_{lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "649a5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['annual', ]\n",
    "locs = ['tract',]\n",
    "start_years = [ 2000]\n",
    "covs = [ '-auto',]\n",
    "models = ['normal', ]\n",
    "learning_rates = [0.1,0.01,0.05]\n",
    "inducing_points = [400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63c7ef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNY\n",
      "0.1\n",
      "0.33628776780490743\n",
      "0.01\n",
      "0.322233869677604\n",
      "0.05\n",
      "0.35320447482645095\n"
     ]
    }
   ],
   "source": [
    "recalc_results = []\n",
    "test_years = 2\n",
    "geography_col='geoid'\n",
    "timestep_col='timestep'\n",
    "outcome_col='deaths'\n",
    "\n",
    "town_map = pd.read_csv(os.path.join(data_dir,'town_tract_map.csv'), dtype=str)\n",
    "group_map = gpd.read_file(os.path.join(data_dir, 'tract_group_map'), dtype=str)\n",
    "\n",
    "\n",
    "# test y always comes from quarterly tract\n",
    "y_timesteps_per_year = 4\n",
    "file_name = f'clean_quarter_tract'\n",
    "data_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "x_idx_cols = [geography_col, 'lat','lon', timestep_col,\n",
    "      'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "      'svi_pctile',\n",
    "      'neighbor_t', 'self_t-1']\n",
    "y_idx_cols = [geography_col, timestep_col, outcome_col]\n",
    "features_only = ['lat','lon', timestep_col,\n",
    "                 'theme_1_pc', 'theme_2_pc', 'theme_3_pc', 'theme_4_pc',\n",
    "                 'svi_pctile',\n",
    "                 'neighbor_t', 'self_t-1']\n",
    "\n",
    "data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "last_train_year = 2018\n",
    "\n",
    "\n",
    "test_y = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                  (data_gdf['year'] <= last_train_year+test_years)][y_idx_cols]\n",
    "starting_y_timestep = int(test_y[timestep_col].min())\n",
    "\n",
    "sorted_y_timesteps = test_y[timestep_col].unique()\n",
    "sorted_y_timesteps.sort()\n",
    "\n",
    "for loc in locs:\n",
    "    for time in times:\n",
    "        x_timesteps_per_year = {'quarter':4, 'semi':2,'annual':1}[time]\n",
    "        \n",
    "        file_name = f'clean_{time}_{loc}'\n",
    "        data_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "        data_gdf = gpd.read_file(data_path)\n",
    "\n",
    "        test_x = data_gdf[(data_gdf['year'] > last_train_year) &\n",
    "                          (data_gdf['year'] <= last_train_year+test_years)][x_idx_cols]\n",
    "        \n",
    "        starting_x_timestep = int(test_x[timestep_col].min())\n",
    "        \n",
    "        test_timesteps_per_year = max(y_timesteps_per_year, x_timesteps_per_year)\n",
    "        test_timesteps = test_timesteps_per_year*test_years\n",
    "        \n",
    "        x_repeats = int(test_timesteps_per_year/x_timesteps_per_year)\n",
    "        y_repeats = int(test_timesteps_per_year/y_timesteps_per_year)\n",
    "        \n",
    "        sorted_x_timesteps = test_x[timestep_col].unique()\n",
    "        sorted_x_timesteps.sort()\n",
    "        \n",
    "        x_timesteps = [timestep  for timestep in sorted_x_timesteps for _ in range(x_repeats)]\n",
    "        \n",
    "        y_timesteps = [timestep  for timestep in sorted_y_timesteps for _ in range(y_repeats)]\n",
    "\n",
    "        for start_year in start_years:\n",
    "            print('HNY')\n",
    "            \n",
    "            for num_inducing in inducing_points:\n",
    "                for model in models:\n",
    "                    for cov in covs:\n",
    "                    \n",
    "                        best_elbo = -np.inf\n",
    "                        bpr_2019 = np.NaN\n",
    "                        bpr_2020 = np.NaN\n",
    "                        \n",
    "                        for lr in learning_rates:\n",
    "                            print(lr)\n",
    "                            this_run = run_template.format(time=time,loc=loc,\n",
    "                                                           model=model,start_year=start_year,\n",
    "                                                           cov=cov,\n",
    "                                                           num_inducing=num_inducing,lr=lr)\n",
    "                            try:\n",
    "                                with open(os.path.join(log_dir,this_run,'model.mod'),'rb') as f:\n",
    "                                        predictor = pickle.load(f)\n",
    "                                with open(os.path.join(log_dir,this_run,'stats.csv'),'rb') as f:\n",
    "                                    stats = pd.read_csv(f)\n",
    "                                    elbo = stats.iloc[-1,:][['elbo']].values[0]\n",
    "                                    \n",
    "                            except(FileNotFoundError):\n",
    "                                #print(f'Broke {this_run}')\n",
    "                                data = [time, loc, start_year, cov, model, num_inducing, np.NaN, np.NaN]\n",
    "                                recalc_results.append(data)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            xtops = []\n",
    "                            for year in range(test_years):\n",
    "                                xtop_year = []\n",
    "                                max_timesteps = max(x_timesteps_per_year, y_timesteps_per_year)\n",
    "                                for x_time,y_time in zip(x_timesteps[year*max_timesteps:(year+1)*max_timesteps],\n",
    "                                                         y_timesteps[year*max_timesteps:(year+1)*max_timesteps]):\n",
    "\n",
    "                                    test_x_time = test_x[test_x[timestep_col] == x_time]\n",
    "                                    test_y_time = test_y[test_y[timestep_col] == y_time]\n",
    "                                    _, _, _, fmean, fvar, gmean, gvar, _, _ = predictor.build_predict(test_x_time.loc[:, features_only].values)\n",
    "                                    g_cond = tf.math.softplus(fmean * normcdf(gmean)).numpy()\n",
    "                                    pred_df = pd.Series(g_cond.squeeze(), index=test_x_time[geography_col])\n",
    "                                    \n",
    "                                    if loc == 'town':\n",
    "                                        merged_to_map = town_map.merge(pred_df.rename(outcome_col), right_index=True,left_on='parent_town', how='right')\n",
    "                                        averaged_over_duplicates = merged_to_map.groupby('child_tracts').mean()[outcome_col]\n",
    "                                        y_index = test_y[geography_col].unique()\n",
    "                                        pred_df = pd.Series(index=y_index,dtype='float64')\n",
    "                                        pred_df.update(averaged_over_duplicates)\n",
    "                                        # not all tracts get mapped from towns\n",
    "                                        pred_df=pred_df.fillna(0)\n",
    "                                    elif loc == 'group':\n",
    "                                        merged_to_map = group_map.merge(pred_df.rename(outcome_col), right_index=True,left_on='grouping', how='right')\n",
    "                                        averaged_over_duplicates = merged_to_map.groupby('geoid').mean()[outcome_col]\n",
    "                                        y_index = test_y[geography_col].unique()\n",
    "                                        pred_df = pd.Series(index=y_index,dtype='float64')\n",
    "                                        pred_df.update(averaged_over_duplicates)\n",
    "                                        \n",
    "                                    xtop_year.append(fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100))\n",
    "                                xtops.append(xtop_year)\n",
    "                            curr_results = {} \n",
    "                            for y, xtop in enumerate( xtops):\n",
    "                                curr_results[f'bpr_100_{y}'] = copy.deepcopy(np.mean([thing[3] for thing in xtop]))\n",
    "                                \n",
    "                            bpr_2019 = copy.copy(curr_results[f'bpr_100_0'])\n",
    "                            bpr_2020 = copy.copy(curr_results[f'bpr_100_1'])\n",
    "                            print(bpr_2019)\n",
    "                                \n",
    "                            if elbo > best_elbo:\n",
    "                                best_bpr_2019 = copy.copy(bpr_2019)\n",
    "                                best_bpr_2020 = copy.copy(bpr_2020)\n",
    "                                best_elbo = elbo\n",
    "                                \n",
    "\n",
    "                        data = [time, loc, start_year, cov, model, num_inducing, best_bpr_2019, best_bpr_2020]\n",
    "                        recalc_results.append(data)\n",
    "\n",
    "#        pd.DataFrame(recalc_results).to_csv(f'~/recalc_fix_results_sheet_{loc}_{time}.csv')\n",
    "#pd.DataFrame(recalc_results).to_csv('~/recalc_fix_results_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c49f7a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(132.0, 0.2826086956521739, 132.0, 0.2826086956521739),\n",
       " (152.0, 0.3183856502242152, 152.0, 0.31838565022421517),\n",
       " (112.0, 0.3672316384180791, 112.0, 0.367231638418079),\n",
       " (139.0, 0.31862745098039214, 139.0, 0.318627450980392)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtop_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d7c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142.0, 0.22826086956521738, 142.0, 0.22826086956521732)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28b65e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(100.0, 0.4152046783625731, 100.0, 0.4152046783625731)],\n",
       " [(142.0, 0.22826086956521738, 142.0, 0.22826086956521732)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7d1467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80.0, 81.0, 82.0, 83.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_timesteps[year*y_timesteps_per_year:(year+1)*y_timesteps_per_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49e0e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtops=[]\n",
    "for year in range(1):\n",
    "    xtop_year = []\n",
    "    for x_time,y_time in zip(x_timesteps[year*max_timesteps:(year+1)*max_timesteps],\n",
    "                             y_timesteps[year*max_timesteps:(year+1)*max_timesteps]):\n",
    "\n",
    "        test_x_time = test_x[test_x[timestep_col] == x_time]\n",
    "        test_y_time = test_y[test_y[timestep_col] == y_time]\n",
    "        _, _, _, fmean, fvar, gmean, gvar, _, _ = predictor.build_predict(test_x_time.loc[:, features_only].values)\n",
    "        g_cond = tf.math.softplus(fmean * normcdf(gmean)).numpy()\n",
    "        pred_df = pd.Series(g_cond.squeeze(), index=test_x_time[geography_col])\n",
    "        xtop_year.append(fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100))\n",
    "    xtops.append(xtop_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd6661d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_229437/1639764889.py\u001b[0m(37)\u001b[0;36mdebug_top_X\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     35 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 37 \u001b[0;31m    bootstrapped_absolute = np.mean([np.abs(top_X_true.sum() - true_qtr_val[indices].sum())\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m                                     for indices in bootstrapped_all_indices])\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m    bootstrapped_ratio = np.mean([np.abs(true_qtr_val[indices]).sum() / np.abs(top_X_true).sum()\n",
      "\u001b[0m\n",
      "ipdb> num_tied_spots\n",
      "1\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "result = fixed_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100)[-1]\n",
    "\n",
    "for i in range(1):\n",
    "    new_result = debug_top_X(test_y_time.set_index(geography_col)[outcome_col], pred_df, 100)[-1]\n",
    "    if new_result != result:\n",
    "        print('did it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62bc8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_top_X(true_qtr_val, pred_qtr_val, X=10):\n",
    "    top_X_predicted = pred_qtr_val.sort_values(ascending=False).iloc[:X]\n",
    "    top_X_true = true_qtr_val.sort_values(ascending=False).iloc[:X]\n",
    "\n",
    "    undisputed_top_predicted = top_X_predicted[top_X_predicted > top_X_predicted.min()]\n",
    "    num_tied_spots = X - len(undisputed_top_predicted)\n",
    "    undisputed_top_true = top_X_true[top_X_true > top_X_true.min()]\n",
    "    num_true_ties = X - len(undisputed_top_true)\n",
    "\n",
    "    tied_top_predicted = pred_qtr_val[pred_qtr_val == top_X_predicted.min()]\n",
    "    tied_top_true = true_qtr_val[true_qtr_val == top_X_true.min()]\n",
    "\n",
    "    error_in_top_true_ties = np.abs(tied_top_true - pred_qtr_val[tied_top_true.index]).sort_values(ascending=True)\n",
    "    error_in_top_pred_ties = np.abs(true_qtr_val[tied_top_predicted.index] - tied_top_predicted).sort_values(\n",
    "        ascending=True)\n",
    "    top_true_tied_geoids = error_in_top_true_ties.iloc[:num_true_ties].index\n",
    "    top_pred_tied_geoids = error_in_top_pred_ties.iloc[:num_tied_spots].index\n",
    "\n",
    "    best_possible_top_true_geoids = pd.Index.union(undisputed_top_true.index, top_true_tied_geoids)\n",
    "    best_possible_top_pred_geoids = pd.Index.union(undisputed_top_predicted.index, top_pred_tied_geoids)\n",
    "\n",
    "    # True values of GEOIDS with highest actual deaths. If ties, finds tied locations that match preds best\n",
    "    best_possible_true = true_qtr_val[best_possible_top_true_geoids]\n",
    "    best_possible_pred = true_qtr_val[best_possible_top_pred_geoids]\n",
    "    assert (len(best_possible_true) == X)\n",
    "    assert (len(best_possible_pred) == X)\n",
    "\n",
    "    best_possible_absolute = np.abs(best_possible_true.sum() - best_possible_pred.sum())\n",
    "    best_possible_ratio = np.abs(best_possible_pred).sum() / np.abs(best_possible_true).sum()\n",
    "\n",
    "    bootstrapped_tied_indices = np.random.choice(tied_top_predicted.index, (1000, num_tied_spots))\n",
    "    bootstrapped_all_indices = [pd.Index.union(undisputed_top_predicted.index,\n",
    "                                               bootstrap_index) for bootstrap_index in bootstrapped_tied_indices]\n",
    "\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    bootstrapped_absolute = np.mean([np.abs(top_X_true.sum() - true_qtr_val[indices].sum())\n",
    "                                     for indices in bootstrapped_all_indices])\n",
    "    bootstrapped_ratio = np.mean([np.abs(true_qtr_val[indices]).sum() / np.abs(top_X_true).sum()\n",
    "                                  for indices in bootstrapped_all_indices])\n",
    "\n",
    "    return best_possible_absolute, best_possible_ratio, bootstrapped_absolute, bootstrapped_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298e346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
